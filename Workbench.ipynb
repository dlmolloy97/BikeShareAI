{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workbench"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.05.2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step was to install ChatGPT's copilot extension.\n",
    "\n",
    "Once this was done, we initially created a simple function to return the current timestamp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function that returns the current time\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "\n",
    "def get_current_time():\n",
    "    return datetime.now()\n",
    "\n",
    "def connect_to_local_postgres_db():\n",
    "    # Create the engine to connect to the PostgreSQL database\n",
    "    engine = create_engine('postgresql://postgres:postgres@localhost:5432/desmondmolloy')\n",
    "    # Create a connection to the engine called `conn`\n",
    "    conn = engine.connect()\n",
    "    # Return the connection\n",
    "    return conn\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this proved successful, it was decided to replicate the classes developed in the local repository.\n",
    "\n",
    "One of the longest and most complex steps had been successfullz copying over the data from the Blue Bikes repository itself. We therefore started by writing a function to extract the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from zipfile import ZipFile\n",
    "\n",
    "def unzip_file_to_local_csv(zip_path):\n",
    "    # Download the zip file from the URL\n",
    "    request.urlretrieve(zip_path, 'data.zip')\n",
    "    # Unzip the file\n",
    "    ZipFile('data.zip').extractall('data')\n",
    "    # Return the unzipped file\n",
    "    # return 'data/tripdata.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and then tested it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "unzip_file_to_local_csv('https://s3.amazonaws.com/hubway-data/202304-bluebikes-tripdata.zip')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It ran successfully. We then inserted the resulting CSV as a table in the local PostGRES database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def insert_csv_as_table_in_local_postgres(csv_path, tablename = 'journeys'):\n",
    "    # Read in the DataFrame from the CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Create a connection to the database\n",
    "    conn = connect_to_local_postgres_db()\n",
    "    # Append the data to the `trips` table\n",
    "    df.to_sql(tablename, conn, index=False, if_exists='append')\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "insert_csv_as_table_in_local_postgres('data/202304-bluebikes-tripdata.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the cell beneath was automatically generated by typing the word \"finally\" in a Python cell."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's create a function that will run all of the functions we've created in order. This will allow us to run a single function to get the data from the URL, unzip it, and insert it into our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_prior_functions():\n",
    "    # Unzip the file\n",
    "    unzip_file_to_local_csv('https://s3.amazonaws.com/hubway-data/202304-bluebikes-tripdata.zip')\n",
    "    # Insert the CSV into the database\n",
    "    insert_csv_as_table_in_local_postgres('data/202304-bluebikes-tripdata.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it was not yet time to take this step. We needed to first create a function to load data from the created table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sql_query_on_postgres_db(query):\n",
    "    # Create a connection to the database\n",
    "    conn = connect_to_local_postgres_db()\n",
    "    # Read the SQL query into a DataFrame\n",
    "    df = pd.read_sql(query, conn)\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "    # Return the DataFrame\n",
    "    return df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was then tested:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>started_at</th>\n",
       "      <th>ended_at</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lng</th>\n",
       "      <th>member_casual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0093AA5E7E3E0158</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2023-04-13 13:49:59</td>\n",
       "      <td>2023-04-13 13:55:04</td>\n",
       "      <td>Innovation Lab - 125 Western Ave at Batten Way</td>\n",
       "      <td>A32011</td>\n",
       "      <td>Soldiers Field Park - 111 Western Ave</td>\n",
       "      <td>A32006</td>\n",
       "      <td>42.363713</td>\n",
       "      <td>-71.124598</td>\n",
       "      <td>42.364263</td>\n",
       "      <td>-71.118276</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BFA8B88E063688F4</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2023-04-25 09:44:38</td>\n",
       "      <td>2023-04-25 09:51:28</td>\n",
       "      <td>Museum of Science</td>\n",
       "      <td>M32045</td>\n",
       "      <td>One Broadway / Kendall Sq at Main St / 3rd St</td>\n",
       "      <td>M32003</td>\n",
       "      <td>42.367690</td>\n",
       "      <td>-71.071163</td>\n",
       "      <td>42.362242</td>\n",
       "      <td>-71.083111</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A9C51FA200C31A81</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2023-04-24 18:39:31</td>\n",
       "      <td>2023-04-24 18:58:05</td>\n",
       "      <td>New Balance - 20 Guest St</td>\n",
       "      <td>D32001</td>\n",
       "      <td>HMS/HSPH - Avenue Louis Pasteur at Longwood Ave</td>\n",
       "      <td>B32003</td>\n",
       "      <td>42.357329</td>\n",
       "      <td>-71.146735</td>\n",
       "      <td>42.337417</td>\n",
       "      <td>-71.102861</td>\n",
       "      <td>casual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0C1D451797FF0871</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2023-04-04 19:25:31</td>\n",
       "      <td>2023-04-04 19:32:14</td>\n",
       "      <td>Museum of Science</td>\n",
       "      <td>M32045</td>\n",
       "      <td>Gore Street at Lambert Street</td>\n",
       "      <td>M32081</td>\n",
       "      <td>42.367690</td>\n",
       "      <td>-71.071163</td>\n",
       "      <td>42.373080</td>\n",
       "      <td>-71.086342</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DDDCD0A2D2EE7A37</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2023-04-11 08:36:14</td>\n",
       "      <td>2023-04-11 08:52:39</td>\n",
       "      <td>Museum of Science</td>\n",
       "      <td>M32045</td>\n",
       "      <td>Columbus Ave at W. Canton St</td>\n",
       "      <td>C32077</td>\n",
       "      <td>42.367690</td>\n",
       "      <td>-71.071163</td>\n",
       "      <td>42.344742</td>\n",
       "      <td>-71.076482</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ride_id rideable_type           started_at             ended_at  \\\n",
       "0  0093AA5E7E3E0158   docked_bike  2023-04-13 13:49:59  2023-04-13 13:55:04   \n",
       "1  BFA8B88E063688F4   docked_bike  2023-04-25 09:44:38  2023-04-25 09:51:28   \n",
       "2  A9C51FA200C31A81   docked_bike  2023-04-24 18:39:31  2023-04-24 18:58:05   \n",
       "3  0C1D451797FF0871   docked_bike  2023-04-04 19:25:31  2023-04-04 19:32:14   \n",
       "4  DDDCD0A2D2EE7A37   docked_bike  2023-04-11 08:36:14  2023-04-11 08:52:39   \n",
       "\n",
       "                               start_station_name start_station_id  \\\n",
       "0  Innovation Lab - 125 Western Ave at Batten Way           A32011   \n",
       "1                               Museum of Science           M32045   \n",
       "2                       New Balance - 20 Guest St           D32001   \n",
       "3                               Museum of Science           M32045   \n",
       "4                               Museum of Science           M32045   \n",
       "\n",
       "                                  end_station_name end_station_id  start_lat  \\\n",
       "0            Soldiers Field Park - 111 Western Ave         A32006  42.363713   \n",
       "1    One Broadway / Kendall Sq at Main St / 3rd St         M32003  42.367690   \n",
       "2  HMS/HSPH - Avenue Louis Pasteur at Longwood Ave         B32003  42.357329   \n",
       "3                    Gore Street at Lambert Street         M32081  42.367690   \n",
       "4                     Columbus Ave at W. Canton St         C32077  42.367690   \n",
       "\n",
       "   start_lng    end_lat    end_lng member_casual  \n",
       "0 -71.124598  42.364263 -71.118276        member  \n",
       "1 -71.071163  42.362242 -71.083111        member  \n",
       "2 -71.146735  42.337417 -71.102861        casual  \n",
       "3 -71.071163  42.373080 -71.086342        member  \n",
       "4 -71.071163  42.344742 -71.076482        member  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_query = \"\"\"\n",
    "SELECT * FROM journeys LIMIT 10;\n",
    "\"\"\"\n",
    "test_df = run_sql_query_on_postgres_db(sql_query)\n",
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_join_to_nearest_station():\n",
    "    # Create the SQL query\n",
    "    sql_query = \"\"\"\n",
    "    SELECT\n",
    "        t.*,\n",
    "        s.station_id,\n",
    "        s.station_name,\n",
    "        s.station_geom\n",
    "    FROM trips AS t\n",
    "    JOIN stations AS s\n",
    "    ON ST_DWithin(t.start_geom, s.station_geom, 100)\n",
    "    LIMIT 10;\n",
    "    \"\"\"\n",
    "    # Run the query and return the DataFrame\n",
    "    return run_sql_query_on_postgres_db(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_csv_as_table_in_local_postgres('data/current_bluebikes_stations.csv', 'stations')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopandas import GeoDataFrame, read_file, points_from_xy\n",
    "import sqldf\n",
    "\n",
    "# Boston neighbourhoods\n",
    "polydf = read_file('data/Boston_Neighborhoods.geojson')\n",
    "\n",
    "stations = run_sql_query_on_postgres_db(\"SELECT * FROM stations\")\n",
    "pointdf = GeoDataFrame(\n",
    "    stations, geometry=points_from_xy(stations.Longitude, stations.Latitude))\n",
    "\n",
    "pointdf.set_crs(epsg='4326', inplace=True)\n",
    "\n",
    "# Make sure they're using the same projection reference\n",
    "\n",
    "#https://geopandas.org/en/stable/gallery/spatial_joins.html\n",
    "joined_df = pointdf.sjoin(polydf, how=\"left\")\n",
    "grab_df = joined_df[['Name_left', 'Name_right', 'District']]\n",
    "matched_pairs = sqldf.run('SELECT DISTINCT Name_left as station, Name_right as neighbourhood from grab_df where District = \\'Boston\\'')\n",
    "matched_pairs.to_csv('data/neighbourhood_stations.csv')\n",
    "insert_csv_as_table_in_local_postgres('data/neighbourhood_stations.csv', 'neighbourhood_stations')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>station</th>\n",
       "      <th>neighbourhood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>175 N Harvard St</td>\n",
       "      <td>Allston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2 Hummingbird Lane at Olmsted Green</td>\n",
       "      <td>Mattapan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>555 Metropolitan Ave</td>\n",
       "      <td>Hyde Park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>606 American Legion Hwy at Canterbury St</td>\n",
       "      <td>Roslindale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>645 Summer St</td>\n",
       "      <td>South Boston Waterfront</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>245</td>\n",
       "      <td>Wentworth Institute of Technology - Huntington...</td>\n",
       "      <td>Fenway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>246</td>\n",
       "      <td>West End Park</td>\n",
       "      <td>West End</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>247</td>\n",
       "      <td>Western Ave at Richardson St</td>\n",
       "      <td>Brighton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>248</td>\n",
       "      <td>Whittier St Health Center</td>\n",
       "      <td>Roxbury</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>249</td>\n",
       "      <td>Williams St at Washington St</td>\n",
       "      <td>Jamaica Plain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                            station  \\\n",
       "0             0                                   175 N Harvard St   \n",
       "1             1                2 Hummingbird Lane at Olmsted Green   \n",
       "2             2                               555 Metropolitan Ave   \n",
       "3             3           606 American Legion Hwy at Canterbury St   \n",
       "4             4                                      645 Summer St   \n",
       "..          ...                                                ...   \n",
       "245         245  Wentworth Institute of Technology - Huntington...   \n",
       "246         246                                      West End Park   \n",
       "247         247                       Western Ave at Richardson St   \n",
       "248         248                          Whittier St Health Center   \n",
       "249         249                       Williams St at Washington St   \n",
       "\n",
       "               neighbourhood  \n",
       "0                    Allston  \n",
       "1                   Mattapan  \n",
       "2                  Hyde Park  \n",
       "3                 Roslindale  \n",
       "4    South Boston Waterfront  \n",
       "..                       ...  \n",
       "245                   Fenway  \n",
       "246                 West End  \n",
       "247                 Brighton  \n",
       "248                  Roxbury  \n",
       "249            Jamaica Plain  \n",
       "\n",
       "[250 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = run_sql_query_on_postgres_db('SELECT * FROM neighbourhood_stations')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join trips and station key using the run_sql_query_on_postgres_db function\n",
    "def insert_df_as_table_in_local_postgres(df, tablename):\n",
    "    # Create a connection to the database\n",
    "    conn = connect_to_local_postgres_db()\n",
    "    # Append the data to the `trips` table\n",
    "    df.to_sql(tablename, conn, index=False, if_exists='append')\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "def join_trips_and_station_key_tables():\n",
    "    # Create the SQL query\n",
    "    sql_query = \"\"\"\n",
    "    SELECT\n",
    "        j.*,\n",
    "        s1.neighbourhood as start_neighbourhood,\n",
    "        s2.neighbourhood as end_neighbourhood\n",
    "    FROM journeys AS j\n",
    "    LEFT JOIN neighbourhood_stations AS s1\n",
    "    ON j.start_station_name = s1.station\n",
    "    LEFT JOIN neighbourhood_stations AS s2\n",
    "    ON j.end_station_name = s2.station\n",
    "\n",
    "    \"\"\"\n",
    "    # Run the query and return the DataFrame\n",
    "    df = run_sql_query_on_postgres_db(sql_query)\n",
    "    insert_df_as_table_in_local_postgres(df, 'journeys_enriched')\n",
    "\n",
    "join_trips_and_station_key_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/7wfwn_sn3cv5zpj5dk7k9r3r0000gn/T/ipykernel_1599/3134024497.py:3: UserWarning: \n",
      "The dash_core_components package is deprecated. Please replace\n",
      "`import dash_core_components as dcc` with `from dash import dcc`\n",
      "  import dash_core_components as dcc\n",
      "/var/folders/63/7wfwn_sn3cv5zpj5dk7k9r3r0000gn/T/ipykernel_1599/3134024497.py:4: UserWarning: \n",
      "The dash_html_components package is deprecated. Please replace\n",
      "`import dash_html_components as html` with `from dash import html`\n",
      "  import dash_html_components as html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m * Tip: There are .env or .flaskenv files present. Do \"pip install python-dotenv\" to use them.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    }
   ],
   "source": [
    "# Write a Dash application in Python to visualise number of bike trips by neighbourhood\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import plotly.express as px    \n",
    "\n",
    "\n",
    "def create_dash_application(group_by='start_neighbourhood'):\n",
    "    # Create the Dash app\n",
    "    app = dash.Dash(__name__)\n",
    "    # Create a DataFrame from the Postgres table\n",
    "    df = run_sql_query_on_postgres_db('SELECT {}, COUNT(*) as journeys_count FROM journeys_enriched group by 1'.format(group_by))\n",
    "    # Create a bar chart of the number of trips by neighbourhood\n",
    "    fig = px.bar(df, x=group_by, y='journeys_count')\n",
    "    # Create the Dash app layout\n",
    "    app.layout = html.Div(children=[\n",
    "        html.H1(children='Hello Dash'),\n",
    "        dcc.Graph(\n",
    "            id='example-graph',\n",
    "            figure=fig\n",
    "        )\n",
    "    ])\n",
    "    # Return the app\n",
    "    return app\n",
    "\n",
    "# Run the application\n",
    "app = create_dash_application()\n",
    "app.run_server(debug=True, use_reloader=False)  # Turn off reloader if inside Jupyter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/7wfwn_sn3cv5zpj5dk7k9r3r0000gn/T/ipykernel_2318/2344893944.py:4: UserWarning: \n",
      "The dash_core_components package is deprecated. Please replace\n",
      "`import dash_core_components as dcc` with `from dash import dcc`\n",
      "  import dash_core_components as dcc\n",
      "/var/folders/63/7wfwn_sn3cv5zpj5dk7k9r3r0000gn/T/ipykernel_2318/2344893944.py:5: UserWarning: \n",
      "The dash_html_components package is deprecated. Please replace\n",
      "`import dash_html_components as html` with `from dash import html`\n",
      "  import dash_html_components as html\n"
     ]
    }
   ],
   "source": [
    "# Create a class that defines the functions defined above as methods, executes them, creates a Dash app, and runs the app\n",
    "\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import plotly.express as px   \n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "from urllib import request\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class BlueBikesDataPipeline:\n",
    "    def __init__(self, group_by='start_neighbourhood'):\n",
    "        self.group_by = group_by\n",
    "\n",
    "    def get_current_time(self):\n",
    "        return datetime.now()\n",
    "    def connect_to_local_postgres_db(self):\n",
    "        # Create the engine to connect to the PostgreSQL database\n",
    "        engine = create_engine('postgresql://postgres:postgres@localhost:5432/desmondmolloy')\n",
    "        # Create a connection to the engine called `conn`\n",
    "        conn = engine.connect()\n",
    "        # Return the connection\n",
    "        return conn\n",
    "    def unzip_file_to_local_csv(self, zip_path):\n",
    "        # Download the zip file from the URL\n",
    "        request.urlretrieve(zip_path, 'data.zip')\n",
    "        # Unzip the file\n",
    "        ZipFile('data.zip').extractall('data')\n",
    "        # Return the unzipped file\n",
    "        # return 'data/tripdata.csv'\n",
    "    def insert_csv_as_table_in_local_postgres(self, csv_path, tablename = 'journeys'):\n",
    "        # Read in the DataFrame from the CSV file\n",
    "        df = pd.read_csv(csv_path)\n",
    "        # Create a connection to the database\n",
    "        conn = connect_to_local_postgres_db()\n",
    "        # Append the data to the `trips` table\n",
    "        df.to_sql(tablename, conn, index=False, if_exists='append')\n",
    "        # Close the connection\n",
    "        conn.close()\n",
    "    def call_prior_functions(self):\n",
    "        # Unzip the file\n",
    "        unzip_file_to_local_csv('https://s3.amazonaws.com/hubway-data/202304-bluebikes-tripdata.zip')\n",
    "        # Insert the CSV into the database\n",
    "        self.insert_csv_as_table_in_local_postgres('data/202304-bluebikes-tripdata.csv')\n",
    "    def run_sql_query_on_postgres_db(self, query):\n",
    "        # Create a connection to the database\n",
    "        conn = self.connect_to_local_postgres_db()\n",
    "        # Read the SQL query into a DataFrame\n",
    "        df = pd.read_sql(query, conn)\n",
    "        # Close the connection\n",
    "        conn.close()\n",
    "        # Return the DataFrame\n",
    "        return df\n",
    "    def insert_df_as_table_in_local_postgres(self, df, tablename):\n",
    "        # Create a connection to the database\n",
    "        conn = self.connect_to_local_postgres_db()\n",
    "        # Append the data to the `trips` table\n",
    "        df.to_sql(tablename, conn, index=False, if_exists='append')\n",
    "        # Close the connection\n",
    "        conn.close()\n",
    "    def join_trips_and_station_key_tables(self):\n",
    "        # Create the SQL query\n",
    "        sql_query = \"\"\"\n",
    "        SELECT\n",
    "            j.*,\n",
    "            s1.neighbourhood as start_neighbourhood,\n",
    "            s2.neighbourhood as end_neighbourhood\n",
    "        FROM journeys AS j\n",
    "        LEFT JOIN neighbourhood_stations AS s1\n",
    "        ON j.start_station_name = s1.station\n",
    "        LEFT JOIN neighbourhood_stations AS s2\n",
    "        ON j.end_station_name = s2.station\n",
    "\n",
    "        \"\"\"\n",
    "        # Run the query and return the DataFrame\n",
    "        df = self.run_sql_query_on_postgres_db(sql_query)\n",
    "        self.insert_df_as_table_in_local_postgres(df, 'journeys_enriched')\n",
    "\n",
    "    def select_all_from_journeys_enriched(self):\n",
    "        return self.run_sql_query_on_postgres_db('SELECT * FROM journeys_enriched')\n",
    "    \n",
    "    def create_dash_application(self):\n",
    "        # Create the Dash app\n",
    "        app = dash.Dash(__name__)\n",
    "        # Create a DataFrame from the Postgres table\n",
    "        df = self.run_sql_query_on_postgres_db('SELECT {}, COUNT(*) as journeys_count FROM journeys_enriched group by 1'.format(self.group_by))\n",
    "        # Create a bar chart of the number of trips by neighbourhood\n",
    "        fig = px.bar(df, x='start_neighbourhood', y='journeys_count')\n",
    "        # Create the Dash app layout\n",
    "        app.layout = html.Div(children=[\n",
    "            html.H1(children='Hello Dash'),\n",
    "            dcc.Graph(\n",
    "                id='example-graph',\n",
    "                figure=fig\n",
    "            )\n",
    "        ])\n",
    "        # Return the app\n",
    "        return app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>started_at</th>\n",
       "      <th>ended_at</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lng</th>\n",
       "      <th>member_casual</th>\n",
       "      <th>start_neighbourhood</th>\n",
       "      <th>end_neighbourhood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0093AA5E7E3E0158</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2023-04-13 13:49:59</td>\n",
       "      <td>2023-04-13 13:55:04</td>\n",
       "      <td>Innovation Lab - 125 Western Ave at Batten Way</td>\n",
       "      <td>A32011</td>\n",
       "      <td>Soldiers Field Park - 111 Western Ave</td>\n",
       "      <td>A32006</td>\n",
       "      <td>42.363713</td>\n",
       "      <td>-71.124598</td>\n",
       "      <td>42.364263</td>\n",
       "      <td>-71.118276</td>\n",
       "      <td>member</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BFA8B88E063688F4</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2023-04-25 09:44:38</td>\n",
       "      <td>2023-04-25 09:51:28</td>\n",
       "      <td>Museum of Science</td>\n",
       "      <td>M32045</td>\n",
       "      <td>One Broadway / Kendall Sq at Main St / 3rd St</td>\n",
       "      <td>M32003</td>\n",
       "      <td>42.367690</td>\n",
       "      <td>-71.071163</td>\n",
       "      <td>42.362242</td>\n",
       "      <td>-71.083111</td>\n",
       "      <td>member</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A9C51FA200C31A81</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2023-04-24 18:39:31</td>\n",
       "      <td>2023-04-24 18:58:05</td>\n",
       "      <td>New Balance - 20 Guest St</td>\n",
       "      <td>D32001</td>\n",
       "      <td>HMS/HSPH - Avenue Louis Pasteur at Longwood Ave</td>\n",
       "      <td>B32003</td>\n",
       "      <td>42.357329</td>\n",
       "      <td>-71.146735</td>\n",
       "      <td>42.337417</td>\n",
       "      <td>-71.102861</td>\n",
       "      <td>casual</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0C1D451797FF0871</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2023-04-04 19:25:31</td>\n",
       "      <td>2023-04-04 19:32:14</td>\n",
       "      <td>Museum of Science</td>\n",
       "      <td>M32045</td>\n",
       "      <td>Gore Street at Lambert Street</td>\n",
       "      <td>M32081</td>\n",
       "      <td>42.367690</td>\n",
       "      <td>-71.071163</td>\n",
       "      <td>42.373080</td>\n",
       "      <td>-71.086342</td>\n",
       "      <td>member</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DDDCD0A2D2EE7A37</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2023-04-11 08:36:14</td>\n",
       "      <td>2023-04-11 08:52:39</td>\n",
       "      <td>Museum of Science</td>\n",
       "      <td>M32045</td>\n",
       "      <td>Columbus Ave at W. Canton St</td>\n",
       "      <td>C32077</td>\n",
       "      <td>42.367690</td>\n",
       "      <td>-71.071163</td>\n",
       "      <td>42.344742</td>\n",
       "      <td>-71.076482</td>\n",
       "      <td>member</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888868</th>\n",
       "      <td>F4ECEA82D2C4B806</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2023-04-18 09:10:33</td>\n",
       "      <td>2023-04-18 09:14:04</td>\n",
       "      <td>MIT at Mass Ave / Amherst St</td>\n",
       "      <td>M32006</td>\n",
       "      <td>Galileo Galilei Way at Main Street</td>\n",
       "      <td>M32072</td>\n",
       "      <td>42.358100</td>\n",
       "      <td>-71.093198</td>\n",
       "      <td>42.363004</td>\n",
       "      <td>-71.089740</td>\n",
       "      <td>member</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888869</th>\n",
       "      <td>2667902845A5247B</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2023-04-18 08:18:38</td>\n",
       "      <td>2023-04-18 08:23:07</td>\n",
       "      <td>MIT at Mass Ave / Amherst St</td>\n",
       "      <td>M32006</td>\n",
       "      <td>Galileo Galilei Way at Main Street</td>\n",
       "      <td>M32072</td>\n",
       "      <td>42.358100</td>\n",
       "      <td>-71.093198</td>\n",
       "      <td>42.363004</td>\n",
       "      <td>-71.089740</td>\n",
       "      <td>member</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888870</th>\n",
       "      <td>D524CCC2E75AD37D</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2023-04-17 11:34:35</td>\n",
       "      <td>2023-04-17 11:49:10</td>\n",
       "      <td>MIT at Mass Ave / Amherst St</td>\n",
       "      <td>M32006</td>\n",
       "      <td>Beacon St at Washington / Kirkland</td>\n",
       "      <td>S32003</td>\n",
       "      <td>42.358100</td>\n",
       "      <td>-71.093198</td>\n",
       "      <td>42.378754</td>\n",
       "      <td>-71.107072</td>\n",
       "      <td>member</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888871</th>\n",
       "      <td>A41CB259754F6EF2</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2023-04-06 17:26:11</td>\n",
       "      <td>2023-04-06 17:34:44</td>\n",
       "      <td>Central Sq Post Office / Cambridge City Hall a...</td>\n",
       "      <td>M32012</td>\n",
       "      <td>Beacon St at Washington / Kirkland</td>\n",
       "      <td>S32003</td>\n",
       "      <td>42.366426</td>\n",
       "      <td>-71.105495</td>\n",
       "      <td>42.378754</td>\n",
       "      <td>-71.107072</td>\n",
       "      <td>member</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888872</th>\n",
       "      <td>8C0DA773DAF5531D</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2023-04-22 12:16:07</td>\n",
       "      <td>2023-04-22 12:23:59</td>\n",
       "      <td>Central Sq Post Office / Cambridge City Hall a...</td>\n",
       "      <td>M32012</td>\n",
       "      <td>Galileo Galilei Way at Main Street</td>\n",
       "      <td>M32072</td>\n",
       "      <td>42.366426</td>\n",
       "      <td>-71.105495</td>\n",
       "      <td>42.363004</td>\n",
       "      <td>-71.089740</td>\n",
       "      <td>member</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>888873 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ride_id rideable_type           started_at  \\\n",
       "0       0093AA5E7E3E0158   docked_bike  2023-04-13 13:49:59   \n",
       "1       BFA8B88E063688F4   docked_bike  2023-04-25 09:44:38   \n",
       "2       A9C51FA200C31A81   docked_bike  2023-04-24 18:39:31   \n",
       "3       0C1D451797FF0871   docked_bike  2023-04-04 19:25:31   \n",
       "4       DDDCD0A2D2EE7A37   docked_bike  2023-04-11 08:36:14   \n",
       "...                  ...           ...                  ...   \n",
       "888868  F4ECEA82D2C4B806   docked_bike  2023-04-18 09:10:33   \n",
       "888869  2667902845A5247B   docked_bike  2023-04-18 08:18:38   \n",
       "888870  D524CCC2E75AD37D   docked_bike  2023-04-17 11:34:35   \n",
       "888871  A41CB259754F6EF2   docked_bike  2023-04-06 17:26:11   \n",
       "888872  8C0DA773DAF5531D   docked_bike  2023-04-22 12:16:07   \n",
       "\n",
       "                   ended_at  \\\n",
       "0       2023-04-13 13:55:04   \n",
       "1       2023-04-25 09:51:28   \n",
       "2       2023-04-24 18:58:05   \n",
       "3       2023-04-04 19:32:14   \n",
       "4       2023-04-11 08:52:39   \n",
       "...                     ...   \n",
       "888868  2023-04-18 09:14:04   \n",
       "888869  2023-04-18 08:23:07   \n",
       "888870  2023-04-17 11:49:10   \n",
       "888871  2023-04-06 17:34:44   \n",
       "888872  2023-04-22 12:23:59   \n",
       "\n",
       "                                       start_station_name start_station_id  \\\n",
       "0          Innovation Lab - 125 Western Ave at Batten Way           A32011   \n",
       "1                                       Museum of Science           M32045   \n",
       "2                               New Balance - 20 Guest St           D32001   \n",
       "3                                       Museum of Science           M32045   \n",
       "4                                       Museum of Science           M32045   \n",
       "...                                                   ...              ...   \n",
       "888868                       MIT at Mass Ave / Amherst St           M32006   \n",
       "888869                       MIT at Mass Ave / Amherst St           M32006   \n",
       "888870                       MIT at Mass Ave / Amherst St           M32006   \n",
       "888871  Central Sq Post Office / Cambridge City Hall a...           M32012   \n",
       "888872  Central Sq Post Office / Cambridge City Hall a...           M32012   \n",
       "\n",
       "                                       end_station_name end_station_id  \\\n",
       "0                 Soldiers Field Park - 111 Western Ave         A32006   \n",
       "1         One Broadway / Kendall Sq at Main St / 3rd St         M32003   \n",
       "2       HMS/HSPH - Avenue Louis Pasteur at Longwood Ave         B32003   \n",
       "3                         Gore Street at Lambert Street         M32081   \n",
       "4                          Columbus Ave at W. Canton St         C32077   \n",
       "...                                                 ...            ...   \n",
       "888868               Galileo Galilei Way at Main Street         M32072   \n",
       "888869               Galileo Galilei Way at Main Street         M32072   \n",
       "888870               Beacon St at Washington / Kirkland         S32003   \n",
       "888871               Beacon St at Washington / Kirkland         S32003   \n",
       "888872               Galileo Galilei Way at Main Street         M32072   \n",
       "\n",
       "        start_lat  start_lng    end_lat    end_lng member_casual  \\\n",
       "0       42.363713 -71.124598  42.364263 -71.118276        member   \n",
       "1       42.367690 -71.071163  42.362242 -71.083111        member   \n",
       "2       42.357329 -71.146735  42.337417 -71.102861        casual   \n",
       "3       42.367690 -71.071163  42.373080 -71.086342        member   \n",
       "4       42.367690 -71.071163  42.344742 -71.076482        member   \n",
       "...           ...        ...        ...        ...           ...   \n",
       "888868  42.358100 -71.093198  42.363004 -71.089740        member   \n",
       "888869  42.358100 -71.093198  42.363004 -71.089740        member   \n",
       "888870  42.358100 -71.093198  42.378754 -71.107072        member   \n",
       "888871  42.366426 -71.105495  42.378754 -71.107072        member   \n",
       "888872  42.366426 -71.105495  42.363004 -71.089740        member   \n",
       "\n",
       "       start_neighbourhood end_neighbourhood  \n",
       "0                     None              None  \n",
       "1                     None              None  \n",
       "2                     None              None  \n",
       "3                     None              None  \n",
       "4                     None              None  \n",
       "...                    ...               ...  \n",
       "888868                None              None  \n",
       "888869                None              None  \n",
       "888870                None              None  \n",
       "888871                None              None  \n",
       "888872                None              None  \n",
       "\n",
       "[888873 rows x 15 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main = BlueBikesDataPipeline()\n",
    "main.select_all_from_journeys_enriched()\n",
    "#main.create_dash_application().run_server(debug=True, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class that executes the functions defined above, creates a Dash app, and runs the app\n",
    "class BlueBikesDashboard:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def run(self):\n",
    "        # Call the functions\n",
    "        call_prior_functions()\n",
    "        spatial_join_to_nearest_station()\n",
    "        join_trips_and_station_key_tables()\n",
    "        # Create the Dash app\n",
    "        app = create_dash_application()\n",
    "        # Run the app\n",
    "        app.run_server(debug=True, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a class that creates a Dash app comparing the number of trips by start and end neighbourhood from the journeys_enriched table\n",
    "class BlueBikesDashboard:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def run(self):\n",
    "        # Create the Dash app\n",
    "        app = create_dash_application()\n",
    "        # Run the app\n",
    "        app.run_server(debug=True, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_dash_application' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m board \u001b[39m=\u001b[39m BlueBikesDashboard()\n\u001b[0;32m----> 2\u001b[0m board\u001b[39m.\u001b[39;49mrun()\n",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m, in \u001b[0;36mBlueBikesDashboard.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m      7\u001b[0m     \u001b[39m# Create the Dash app\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     app \u001b[39m=\u001b[39m create_dash_application()\n\u001b[1;32m      9\u001b[0m     \u001b[39m# Run the app\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     app\u001b[39m.\u001b[39mrun_server(debug\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, use_reloader\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_dash_application' is not defined"
     ]
    }
   ],
   "source": [
    "board = BlueBikesDashboard()\n",
    "board.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/7wfwn_sn3cv5zpj5dk7k9r3r0000gn/T/ipykernel_2614/2131602213.py:9: UserWarning: \n",
      "The dash_core_components package is deprecated. Please replace\n",
      "`import dash_core_components as dcc` with `from dash import dcc`\n",
      "  import dash_core_components as dcc\n",
      "/var/folders/63/7wfwn_sn3cv5zpj5dk7k9r3r0000gn/T/ipykernel_2614/2131602213.py:10: UserWarning: \n",
      "The dash_html_components package is deprecated. Please replace\n",
      "`import dash_html_components as html` with `from dash import html`\n",
      "  import dash_html_components as html\n"
     ]
    }
   ],
   "source": [
    "# Create class with the following methods:\n",
    "# - init: create a connection to the local postgres database\n",
    "# - run_sql_query_on_postgres_db: run a SQL query on the database and return a DataFrame\n",
    "# - create_dash_application: create a Dash app that displays the number of trips by start and end neighbourhood\n",
    "# - run: call the create_dash_application method and run the app\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import plotly.express as px\n",
    "\n",
    "class BlueBikesDashboard:\n",
    "\n",
    "    def __init__(self):\n",
    "        engine = create_engine('postgresql://postgres:postgres@localhost:5432/desmondmolloy')\n",
    "        # Create a connection to the engine called `conn`\n",
    "        self.conn = engine.connect()\n",
    "    \n",
    "    def run_sql_query_on_postgres_db(self, query):\n",
    "        # Read the SQL query into a DataFrame\n",
    "        df = pd.read_sql(query, self.conn)\n",
    "        # Return the DataFrame\n",
    "        return df\n",
    "    def join_trips_and_station_key_tables(self):\n",
    "        # Create the SQL query\n",
    "        sql_query = \"\"\"\n",
    "        SELECT\n",
    "            j.*,\n",
    "            s1.neighbourhood as start_neighbourhood,\n",
    "            s2.neighbourhood as end_neighbourhood\n",
    "        FROM journeys AS j\n",
    "        LEFT JOIN neighbourhood_stations AS s1\n",
    "        ON j.start_station_name = s1.station\n",
    "        LEFT JOIN neighbourhood_stations AS s2\n",
    "        ON j.end_station_name = s2.station\n",
    "\n",
    "        \"\"\"\n",
    "        # Run the query and return the DataFrame\n",
    "        df = self.run_sql_query_on_postgres_db(sql_query)\n",
    "        self.insert_df_as_table_in_local_postgres(df, 'journeys_enriched')\n",
    "        \n",
    "    def create_dash_application(self, group_by='start_neighbourhood'):\n",
    "        # Create the Dash app\n",
    "        app = dash.Dash(__name__)\n",
    "        # Create a DataFrame from the Postgres table\n",
    "        df = self.run_sql_query_on_postgres_db('SELECT {}, COUNT(*) as journeys_count FROM journeys_enriched group by 1'.format(group_by))\n",
    "        # Create a bar chart of the number of trips by neighbourhood\n",
    "        fig = px.bar(df, x='start_neighbourhood', y='journeys_count')\n",
    "        # Create the Dash app layout\n",
    "        app.layout = html.Div(children=[\n",
    "            html.H1(children='Hello Dash'),\n",
    "            dcc.Graph(\n",
    "                id='example-graph',\n",
    "                figure=fig\n",
    "            )\n",
    "        ])\n",
    "        # Return the app\n",
    "        return app\n",
    "    def run(self):\n",
    "        # Create the Dash app\n",
    "        app = self.create_dash_application()\n",
    "        # Run the app\n",
    "        app.run_server(debug=True, use_reloader=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Create an instance of BlueBikesDashboard and run the app  \n",
    "board = BlueBikesDashboard()\n",
    "board.join_trips_and_station_key_tables()\n",
    "board.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing bikeshareai/bluebikes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bikeshareai/bluebikes.py\n",
    "# Create two classes:\n",
    "# The first one should include the following methods:\n",
    "# __init__: Takes a URL of a zip file as argument, and unzips the file into a CSV in the data folder\n",
    "# connect_to_db: Creates and returns a connection to the local PostgreSQL database using the sqlalchemy package.\n",
    "# csv_to_db: Takes the arguments table_name and csv_path. If no table with the name table_name exists in the local PostgreSQL, then insert the csv located at csv_path as a table with the name table_name\n",
    "# geojoin: Takes the argument geojson_path. Runs the csv_to_db method on 'data/current_bluebikes_stations.csv', with stations as the table_name. Loads the data from geojson path as polydf, loads the table stations as a Geopandas GeoDataFrame with epsg='4326', and then performs a spatial join between polydf and the stations GeoDataFrame. The resulting Dataframe should be written to the local PostgreSQL database as the table neighbourhood_stations\n",
    "# enrich_journeys: This method joins the journeys and neighbourhood_stations tables using journeys.start_station_name = neighbourhood_stations.station twice, producing a table with all of the columns from journeys, plus the start and end neighbourhoods and stations for each journey, creating a table in the PostgreSQL database called journeys_enriched\n",
    "# The second one should include the following methods:\n",
    "# - create_dash_application: create a Dash app that displays the number of trips by start and end neighbourhood, from the journeys_enriched table\n",
    "# - run: call the create_dash_application method and run the app\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from urllib import request\n",
    "from zipfile import ZipFile\n",
    "from geopandas import GeoDataFrame, read_file, points_from_xy\n",
    "import sqldf\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import plotly.express as px\n",
    "\n",
    "class BlueBikesDataPipeline:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.engine = create_engine('postgresql://postgres:postgres@localhost:5432/desmondmolloy')\n",
    "        # Create a connection to the engine called `conn`\n",
    "        self.conn = self.engine.connect()\n",
    "    def unzip_file_to_local_csv(self):\n",
    "        # Download the zip file from the URL\n",
    "        request.urlretrieve(self.url, 'data.zip')\n",
    "        # Unzip the file\n",
    "        ZipFile('data.zip').extractall('data')\n",
    "        # Return the unzipped file\n",
    "        # return 'data/tripdata.csv'\n",
    "    def csv_to_db(self, table_name, csv_path):\n",
    "        # Read in the DataFrame from the CSV file\n",
    "        df = pd.read_csv(csv_path)\n",
    "        # Append the data to the `trips` table\n",
    "        df.to_sql(table_name, self.conn, index=False, if_exists='append')\n",
    "    def geojoin(self, geojson_path):\n",
    "        # Read in the DataFrame from the CSV file\n",
    "        df = pd.read_csv('data/current_bluebikes_stations.csv')\n",
    "        # Append the data to the `trips` table\n",
    "        df.to_sql('stations', self.conn, index=False, if_exists='append')\n",
    "        # Boston neighbourhoods\n",
    "        polydf = read_file(geojson_path)\n",
    "        stations = pd.read_sql('SELECT * FROM stations', self.conn)\n",
    "        pointdf = GeoDataFrame(\n",
    "            stations, geometry=points_from_xy(stations.Longitude, stations.Latitude))\n",
    "        pointdf.set_crs(epsg='4326', inplace=True)\n",
    "        joined_df = pointdf.sjoin(polydf, how=\"left\")\n",
    "        grab_df = joined_df[['Name_left', 'Name_right', 'District']]\n",
    "        matched_pairs = sqldf.run('SELECT DISTINCT Name_left as station, Name_right as neighbourhood from grab_df where District = \\'Boston\\'')\n",
    "        matched_pairs.to_sql('neighbourhood_stations', self.conn, index=False, if_exists='append')\n",
    "\n",
    "    def enrich_journeys(self):\n",
    "        # Create the SQL query\n",
    "        sql_query = \"\"\"\n",
    "        SELECT\n",
    "            j.*,\n",
    "            s1.neighbourhood as start_neighbourhood,\n",
    "            s2.neighbourhood as end_neighbourhood\n",
    "        FROM journeys AS j\n",
    "        LEFT JOIN neighbourhood_stations AS s1\n",
    "        ON j.start_station_name = s1.station\n",
    "        LEFT JOIN neighbourhood_stations AS s2\n",
    "        ON j.end_station_name = s2.station\n",
    "\n",
    "        \"\"\"\n",
    "        # Run the query and return the DataFrame\n",
    "        df = pd.read_sql(sql_query, self.conn)\n",
    "        df.to_sql('journeys_enriched', self.conn, index=False, if_exists='append')\n",
    "\n",
    "class BlueBikesDashboard:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def create_dash_application(self, group_by='start_neighbourhood'):\n",
    "        # Create the Dash app\n",
    "        app = dash.Dash(__name__)\n",
    "        # Create a DataFrame from the Postgres table\n",
    "        df = pd.read_sql('SELECT {}, COUNT(*) as journeys_count FROM journeys_enriched group by 1'.format(group_by), self.conn)\n",
    "        # Create a bar chart of the number of trips by neighbourhood\n",
    "        fig = px.bar(df, x='start_neighbourhood', y='journeys_count')\n",
    "        # Create the Dash app layout\n",
    "        app.layout = html.Div(children=[\n",
    "            html.H1(children='Hello Dash'),\n",
    "            dcc.Graph(\n",
    "                id='example-graph',\n",
    "                figure=fig\n",
    "            )\n",
    "        ])\n",
    "        # Return the app\n",
    "        return app\n",
    "    def run(self):\n",
    "        # Create the Dash app\n",
    "        app = self.create_dash_application()\n",
    "        # Run the app\n",
    "        app.run_server(debug=True, use_reloader=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/desmondmolloy/Documents/GitHub/BikeShareAI/bikeshareai/bluebikes.py:18: UserWarning: \n",
      "The dash_core_components package is deprecated. Please replace\n",
      "`import dash_core_components as dcc` with `from dash import dcc`\n",
      "  import dash_core_components as dcc\n",
      "/Users/desmondmolloy/Documents/GitHub/BikeShareAI/bikeshareai/bluebikes.py:19: UserWarning: \n",
      "The dash_html_components package is deprecated. Please replace\n",
      "`import dash_html_components as html` with `from dash import html`\n",
      "  import dash_html_components as html\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module '__main__' has no attribute 'grab_df'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m pipeline\u001b[39m.\u001b[39munzip_file_to_local_csv()\n\u001b[1;32m      5\u001b[0m pipeline\u001b[39m.\u001b[39mcsv_to_db(\u001b[39m'\u001b[39m\u001b[39mjourneys\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdata/202304-bluebikes-tripdata.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m pipeline\u001b[39m.\u001b[39;49mgeojoin(\u001b[39m'\u001b[39;49m\u001b[39mdata/Boston_Neighborhoods.geojson\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m pipeline\u001b[39m.\u001b[39menrich_journeys()\n\u001b[1;32m      8\u001b[0m \u001b[39m# Create an instance of BlueBikesDashboard and run the app\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/BikeShareAI/bikeshareai/bluebikes.py:53\u001b[0m, in \u001b[0;36mBlueBikesDataPipeline.geojoin\u001b[0;34m(self, geojson_path)\u001b[0m\n\u001b[1;32m     51\u001b[0m joined_df \u001b[39m=\u001b[39m pointdf\u001b[39m.\u001b[39msjoin(polydf, how\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m grab_df \u001b[39m=\u001b[39m joined_df[[\u001b[39m'\u001b[39m\u001b[39mName_left\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mName_right\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mDistrict\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m---> 53\u001b[0m matched_pairs \u001b[39m=\u001b[39m sqldf\u001b[39m.\u001b[39;49mrun(\u001b[39m'\u001b[39;49m\u001b[39mSELECT DISTINCT Name_left as station, Name_right as neighbourhood from grab_df where District = \u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39mBoston\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     54\u001b[0m matched_pairs\u001b[39m.\u001b[39mto_sql(\u001b[39m'\u001b[39m\u001b[39mneighbourhood_stations\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconn, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, if_exists\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mappend\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/BikeShareAI/.conda/lib/python3.11/site-packages/sqldf/sqldf.py:120\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mfor\u001b[39;00m dataframe \u001b[39min\u001b[39;00m df_to_virtualize:\n\u001b[1;32m    119\u001b[0m     exec(\u001b[39m\"\u001b[39m\u001b[39mglobal \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m dataframe)\n\u001b[0;32m--> 120\u001b[0m     temp_df \u001b[39m=\u001b[39m \u001b[39meval\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39m__main__.\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m+\u001b[39;49mdataframe)\n\u001b[1;32m    121\u001b[0m     temp_df\u001b[39m.\u001b[39mto_sql(dataframe, conn)\n\u001b[1;32m    122\u001b[0m     \u001b[39m# Save the name of the index\u001b[39;00m\n",
      "File \u001b[0;32m<string>:1\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module '__main__' has no attribute 'grab_df'"
     ]
    }
   ],
   "source": [
    "from bikeshareai.bluebikes import BlueBikesDataPipeline, BlueBikesDashboard\n",
    "# Create an instance of BlueBikesDataPipeline and run the methods\n",
    "pipeline = BlueBikesDataPipeline('https://s3.amazonaws.com/hubway-data/202304-bluebikes-tripdata.zip')\n",
    "pipeline.unzip_file_to_local_csv()\n",
    "pipeline.csv_to_db('journeys', 'data/202304-bluebikes-tripdata.csv')\n",
    "pipeline.geojoin('data/Boston_Neighborhoods.geojson')\n",
    "pipeline.enrich_journeys()\n",
    "# Create an instance of BlueBikesDashboard and run the app\n",
    "board = BlueBikesDashboard()\n",
    "board.run()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The geojoin class had to be modified by hand, as sqldf cannot be run within a function. We therefore removed it from the imported packages to see if this would induce Copilot to choose a different joining method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bikeshareai/bluebikes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bikeshareai/bluebikes.py\n",
    "# Create two classes:\n",
    "# The first one should include the following methods:\n",
    "# __init__: Takes a URL of a zip file as argument, and unzips the file into a CSV in the data folder\n",
    "# connect_to_db: Creates and returns a connection to the local PostgreSQL database using the sqlalchemy package.\n",
    "# csv_to_db: Takes the arguments table_name and csv_path. If no table with the name table_name exists in the local PostgreSQL, then insert the csv located at csv_path as a table with the name table_name\n",
    "# geojoin: Takes the argument geojson_path. Runs the csv_to_db method on 'data/current_bluebikes_stations.csv', with stations as the table_name. Loads the data from geojson path as polydf, loads the table stations as a Geopandas GeoDataFrame with epsg='4326', and then performs a spatial join between polydf and the stations GeoDataFrame, using pandas to select the stations and neighbourhood for stations in Boston. The resulting Dataframe should be written to the local PostgreSQL database as the table neighbourhood_stations\n",
    "# enrich_journeys: This method joins the journeys and neighbourhood_stations tables using journeys.start_station_name = neighbourhood_stations.station twice, producing a table with all of the columns from journeys, plus the start and end neighbourhoods and stations for each journey, creating a table in the PostgreSQL database called journeys_enriched\n",
    "# The second one should include the following methods:\n",
    "# - create_dash_application: create a Dash app that displays the number of trips by start and end neighbourhood, from the journeys_enriched table\n",
    "# - run: call the create_dash_application method and run the app\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from urllib import request\n",
    "from zipfile import ZipFile\n",
    "from geopandas import GeoDataFrame, read_file, points_from_xy\n",
    "import dash\n",
    "#import dash_core_components as dcc\n",
    "from dash import dcc\n",
    "#import dash_html_components as html\n",
    "from dash import html\n",
    "import plotly.express as px\n",
    "\n",
    "class BlueBikesDataPipeline:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.engine = create_engine('postgresql://postgres:postgres@localhost:5432/desmondmolloy')\n",
    "        # Create a connection to the engine called `conn`\n",
    "        self.conn = self.engine.connect()\n",
    "    \n",
    "    def unzip_file_to_local_csv(self):\n",
    "        # Download the zip file from the URL\n",
    "        request.urlretrieve(self.url, 'data.zip')\n",
    "        # Unzip the file\n",
    "        ZipFile('data.zip').extractall('data')\n",
    "        # Return the unzipped file\n",
    "        # return 'data/tripdata.csv'\n",
    "    \n",
    "    def csv_to_db(self, table_name, csv_path):\n",
    "        # Read in the DataFrame from the CSV file\n",
    "        df = pd.read_csv(csv_path)\n",
    "        # Append the data to the `trips` table\n",
    "        df.to_sql(table_name, self.conn, index=False, if_exists='append')\n",
    "    \n",
    "    def geojoin(self, geojson_path):\n",
    "        # Read in the DataFrame from the CSV file\n",
    "        df = pd.read_csv('data/current_bluebikes_stations.csv')\n",
    "        # Append the data to the `trips` table\n",
    "        df.to_sql('stations', self.conn, index=False, if_exists='append')\n",
    "        # Boston neighbourhoods\n",
    "        polydf = read_file(geojson_path)\n",
    "        stations = pd.read_sql('SELECT * FROM stations', self.conn)\n",
    "        pointdf = GeoDataFrame(\n",
    "            stations, geometry=points_from_xy(stations.Longitude, stations.Latitude))\n",
    "        pointdf.set_crs(epsg='4326', inplace=True)\n",
    "        joined_df = pointdf.sjoin(polydf, how=\"left\")\n",
    "        grab_df = joined_df[['Name_left', 'Name_right', 'District']]\n",
    "        matched_pairs_with_pandas = grab_df[grab_df['District'] == 'Boston']\n",
    "        matched_pairs_with_pandas.columns = ['station', 'neighbourhood', 'District']\n",
    "        #matched_pairs = sqldf.run('SELECT DISTINCT Name_left as station, Name_right as neighbourhood from grab_df where District = \\'Boston\\'')\n",
    "        matched_pairs_with_pandas.to_sql('neighbourhood_stations', self.conn, index=False, if_exists='append')\n",
    "\n",
    "    def enrich_journeys(self):\n",
    "        # Create the SQL query\n",
    "        sql_query = \"\"\"\n",
    "        SELECT\n",
    "            j.*,\n",
    "            s1.neighbourhood as start_neighbourhood,\n",
    "            s2.neighbourhood as end_neighbourhood\n",
    "        FROM journeys AS j\n",
    "        LEFT JOIN neighbourhood_stations AS s1\n",
    "        ON j.start_station_name = s1.station\n",
    "        LEFT JOIN neighbourhood_stations AS s2\n",
    "        ON j.end_station_name = s2.station\n",
    "\n",
    "        \"\"\"\n",
    "        # Run the query and return the DataFrame\n",
    "        df = pd.read_sql(sql_query, self.conn)\n",
    "        df.to_sql('journeys_enriched', self.conn, index=False, if_exists='append')\n",
    "    \n",
    "class BlueBikesDashboard:\n",
    "    def __init__(self):\n",
    "        self.engine = create_engine('postgresql://postgres:postgres@localhost:5432/desmondmolloy')\n",
    "        # Create a connection to the engine called `conn`\n",
    "        self.conn = self.engine.connect()\n",
    "    def create_dash_application(self, group_by='start_neighbourhood'):\n",
    "        # Create the Dash app\n",
    "        app = dash.Dash(__name__)\n",
    "        # Create a DataFrame from the Postgres table\n",
    "        df = pd.read_sql('SELECT {}, COUNT(*) as journeys_count FROM journeys_enriched group by 1'.format(group_by), self.conn)\n",
    "        # Create a bar chart of the number of trips by neighbourhood\n",
    "        fig = px.bar(df, x='start_neighbourhood', y='journeys_count')\n",
    "        # Create the Dash app layout\n",
    "        app.layout = html.Div(children=[\n",
    "            html.H1(children='Hello Dash'),\n",
    "            dcc.Graph(\n",
    "                id='example-graph',\n",
    "                figure=fig\n",
    "            )\n",
    "        ])\n",
    "        # Return the app\n",
    "        return app\n",
    "    def run(self):\n",
    "        # Create the Dash app\n",
    "        app = self.create_dash_application()\n",
    "        # Run the app\n",
    "        app.run_server(debug=True, use_reloader=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling for \"matched pairs with pandas\" enabled Copilot to skip the hurdle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bikeshareai.bluebikes import BlueBikesDataPipeline, BlueBikesDashboard\n",
    "# Create an instance of BlueBikesDataPipeline and run the methods\n",
    "pipeline = BlueBikesDataPipeline('https://s3.amazonaws.com/hubway-data/202304-bluebikes-tripdata.zip')\n",
    "pipeline.unzip_file_to_local_csv()\n",
    "pipeline.csv_to_db('journeys', 'data/202304-bluebikes-tripdata.csv')\n",
    "pipeline.geojoin('data/Boston_Neighborhoods.geojson')\n",
    "pipeline.enrich_journeys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m * Tip: There are .env or .flaskenv files present. Do \"pip install python-dotenv\" to use them.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of BlueBikesDashboard and run the app\n",
    "board = BlueBikesDashboard()\n",
    "board.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21.05.2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The design objectives are as follows:\n",
    "1. Create a DAG that performs the DataPipeline step if table journeys_enriched is not present, and then proceeds to BlueBikesDashboard\n",
    "2. Use Spark to parallelise the steps performed with DataPipeline. This will be the most difficult step, as I am less familiar with the pyspark library, and had not reached this step in the original project yet. It may be necessary simply to dive in here and see what Copilot produces as inspiration.\n",
    "    1. On the other hand, we can also examine which steps are most Python - intensive. Geojoin is mostly performed locally in Python, which makes it a potentially fruitful target for PySparkification\n",
    "3. Make the Dash app more interactive, with the following features:\n",
    "    1. Enable user to look at either number of journeys or journey length\n",
    "    2. Add a select option that enables them to group by either start or end neighbourhood, day of the week, hour of the day or month of the year\n",
    "4. Create a README for the miniproject"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first (relatively lightweight) set of instructions is beneath:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two classes:\n",
    "# The first one should include the following methods:\n",
    "# __init__: Takes a URL of a zip file as argument, and unzips the file into a CSV in the data folder\n",
    "# connect_to_db: Creates and returns a connection to the local PostgreSQL database using the sqlalchemy package.\n",
    "# csv_to_db: Takes the arguments table_name and csv_path. If no table with the name table_name exists in the local PostgreSQL, then insert the csv located at csv_path as a table with the name table_name\n",
    "# main_join: Takes the arguments geojson_path, station_path, and journeys_path, and creates a PySpark context to carry out the following tasks using Spark SQL:\n",
    "# Loads the csv at station_path as a dataframe called stations, and the csv at journeys_path as journeys. Loads the data from geojson path as polydf, loads the table stations as a Geopandas GeoDataFrame with epsg='4326', and then performs a spatial join between polydf and the stations GeoDataFrame. \n",
    "# The resulting Dataframe should be joined to the journey dataframe using journeys.start_station_name = neighbourhood_stations.station twice, producing a table with all of the columns from journeys, plus the start and end neighbourhoods and stations for each journey. Finally, the resulting DataFrame should be written to the PostgreSQL database as a table called journeys_enriched\n",
    "# The second one should include the following methods:\n",
    "# - create_dash_application: create a Dash application. In this application, \n",
    "#it should be possible to select the number of journeys or the average duration as response_variable, and then summarise response_variable by any of the following variables:\n",
    "# start_neighbourhood\n",
    "# end_neighbourhood\n",
    "# day_of_week\n",
    "# hour_of_day\n",
    "# month_of_year\n",
    "# - run: call the create_dash_application method and run the app\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from urllib import request\n",
    "from zipfile import ZipFile\n",
    "from geopandas import GeoDataFrame, read_file, points_from_xy\n",
    "import dash\n",
    "#import dash_core_components as dcc\n",
    "from dash import dcc\n",
    "#import dash_html_components as html\n",
    "from dash import html\n",
    "import plotly.express as px\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType, TimestampType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, dayofweek\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "class BlueBikesDataPipeline:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.engine = create_engine('postgresql://postgres:postgres@localhost:5432/desmondmolloy')\n",
    "        # Create a connection to the engine called `conn`\n",
    "        self.conn = self.engine.connect()\n",
    "\n",
    "    def unzip_file_to_local_csv(self):\n",
    "        # Download the zip file from the URL\n",
    "        request.urlretrieve(self.url, 'data.zip')\n",
    "        # Unzip the file\n",
    "        ZipFile('data.zip').extractall('data')\n",
    "        # Return the unzipped file\n",
    "        # return 'data/tripdata.csv'\n",
    "\n",
    "    def csv_to_db(self, table_name, csv_path):\n",
    "        # Read in the DataFrame from the CSV file\n",
    "        df = pd.read_csv(csv_path)\n",
    "        # Append the data to the `trips` table\n",
    "        df.to_sql(table_name, self.conn, index=False, if_exists='append')\n",
    "    \n",
    "    def main_join(self, geojson_path, station_path, journeys_path):\n",
    "        poly_df = read_file(geojson_path)\n",
    "        poly_df = poly_df[poly_df['District'] == 'Boston']\n",
    "        poly_df = poly_df[['Name', 'geometry']]\n",
    "        poly_df.columns = ['neighbourhood', 'geometry']\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.lower()\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace(' ', '_')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('-', '_')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('\\'', '')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('(', '')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace(')', '')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('.', '')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('&', 'and')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('é', 'e')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('ö', 'o')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copilot got stuck in a loop of suggesting character replacements (see above). To break out of this, it was necessary to add code for a next step manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two classes:\n",
    "# The first one should include the following methods:\n",
    "# __init__: Takes a URL of a zip file as argument, and unzips the file into a CSV in the data folder\n",
    "# connect_to_db: Creates and returns a connection to the local PostgreSQL database using the sqlalchemy package.\n",
    "# df_to_db: Takes the arguments table_name and dataframe. If no table with the name table_name exists in the local PostgreSQL, then insert the csv located at csv_path as a table with the name table_name\n",
    "# main_join: Takes the arguments geojson_path, station_path, and journeys_path, and creates a PySpark context to carry out the following tasks using Spark SQL:\n",
    "# Loads the csv at station_path as a dataframe called stations, and the csv at journeys_path as journeys. Loads the data from geojson path as polydf, loads the table stations as a Geopandas GeoDataFrame with epsg='4326', and then performs a spatial join between polydf and the stations GeoDataFrame. \n",
    "# The resulting Dataframe should be joined to the journey dataframe using journeys.start_station_name = neighbourhood_stations.station twice, producing a table with all of the columns from journeys, plus the start and end neighbourhoods and stations for each journey. Finally, the resulting DataFrame should be written to the PostgreSQL database as a table called journeys_enriched\n",
    "# The second one should include the following methods:\n",
    "# - create_dash_application: create a Dash application. In this application, \n",
    "#it should be possible to select the number of journeys or the average duration as response_variable, and then summarise response_variable by any of the following variables:\n",
    "# start_neighbourhood\n",
    "# end_neighbourhood\n",
    "# day_of_week\n",
    "# hour_of_day\n",
    "# month_of_year\n",
    "# - run: call the create_dash_application method and run the app\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from urllib import request\n",
    "from zipfile import ZipFile\n",
    "from geopandas import GeoDataFrame, read_file, points_from_xy\n",
    "import dash\n",
    "#import dash_core_components as dcc\n",
    "from dash import dcc\n",
    "#import dash_html_components as html\n",
    "from dash import html\n",
    "import plotly.express as px\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType, TimestampType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, dayofweek\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "class BlueBikesDataPipeline:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.engine = create_engine('postgresql://postgres:postgres@localhost:5432/desmondmolloy')\n",
    "        # Create a connection to the engine called `conn`\n",
    "        self.conn = self.engine.connect()\n",
    "\n",
    "    def unzip_file_to_local_csv(self):\n",
    "        # Download the zip file from the URL\n",
    "        request.urlretrieve(self.url, 'data.zip')\n",
    "        # Unzip the file\n",
    "        ZipFile('data.zip').extractall('data')\n",
    "        # Return the unzipped file\n",
    "        # return 'data/tripdata.csv'\n",
    "    \n",
    "    def df_to_db(self, table_name, dataframe):\n",
    "        # Append the data to the `trips` table\n",
    "        dataframe.to_sql(table_name, self.conn, index=False, if_exists='append')\n",
    "\n",
    "\n",
    "    def csv_to_db(self, table_name, csv_path):\n",
    "        # Read in the DataFrame from the CSV file\n",
    "        df = pd.read_csv(csv_path)\n",
    "        # Append the data to the `trips` table\n",
    "        df.to_sql(table_name, self.conn, index=False, if_exists='append')\n",
    "    \n",
    "    def main_join(self, geojson_path, station_path, journeys_path):\n",
    "        poly_df = read_file(geojson_path)\n",
    "        poly_df = poly_df[poly_df['District'] == 'Boston']\n",
    "        poly_df = poly_df[['Name', 'geometry']]\n",
    "        poly_df.columns = ['neighbourhood', 'geometry']\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.lower()\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace(' ', '_')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('-', '_')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('\\'', '')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('(', '')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace(')', '')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('.', '')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('&', 'and')\n",
    "        # Here is where manual was added\n",
    "        stations = pd.read_csv(station_path)\n",
    "        stations = stations[['Station ID', 'Station Name', 'Latitude', 'Longitude']]\n",
    "        stations.columns = ['station_id', 'station_name', 'latitude', 'longitude']\n",
    "        stations['station_name'] = stations['station_name'].str.lower()\n",
    "        stations['station_name'] = stations['station_name'].str.replace(' ', '_')\n",
    "        stations['station_name'] = stations['station_name'].str.replace('-', '_')\n",
    "        stations['station_name'] = stations['station_name'].str.replace('\\'', '')\n",
    "        stations['station_name'] = stations['station_name'].str.replace('(', '')\n",
    "        stations['station_name'] = stations['station_name'].str.replace(')', '')\n",
    "        stations['station_name'] = stations['station_name'].str.replace('.', '')\n",
    "        stations['station_name'] = stations['station_name'].str.replace('&', 'and')\n",
    "        # Here, the loop began again, forcing manual changes\n",
    "        stations_geo = GeoDataFrame(stations, geometry=points_from_xy(stations.longitude, stations.latitude))\n",
    "        stations_geo.set_crs(epsg='4326', inplace=True)\n",
    "        joined_df = stations_geo.sjoin(poly_df, how=\"left\")\n",
    "        grab_df = joined_df[['station_name', 'neighbourhood']]\n",
    "        matched_pairs_with_pandas = grab_df[grab_df['neighbourhood'] == 'boston']\n",
    "        matched_pairs_with_pandas.columns = ['station', 'neighbourhood']\n",
    "        matched_pairs_with_pandas.to_sql('neighbourhood_stations', self.conn, index=False, if_exists='append')\n",
    "        # Here, the loop began again, forcing manual changes\n",
    "        spark = SparkSession.builder.appName(\"BlueBikes\").getOrCreate()\n",
    "        schema = StructType([])\n",
    "        journeys = spark.read.csv(journeys_path, header=True, schema=schema)\n",
    "        journeys = journeys.withColumnRenamed('starttime', 'start_time')\n",
    "        journeys = journeys.withColumnRenamed('stoptime', 'end_time')\n",
    "        #journeys = journeys.withColumnRenamed('start station id', 'start_station_id')\n",
    "        #journeys = journeys.withColumnRenamed('end station id', 'end_station_id')\n",
    "        #journeys = journeys.withColumnRenamed('start station name', 'start_station_name')\n",
    "        #journeys = journeys.withColumnRenamed('end station name', 'end_station_name')\n",
    "        #journeys = journeys.withColumnRenamed('usertype', 'user_type')\n",
    "        #journeys = journeys.withColumnRenamed('birth year', 'birth_year')\n",
    "        #Suugestions died here\n",
    "        journeys = journeys.withColumn('start_time', to_timestamp(journeys.start_time, 'MM/dd/yyyy HH:mm'))\n",
    "        journeys = journeys.withColumn('end_time', to_timestamp(journeys.end_time, 'MM/dd/yyyy HH:mm'))\n",
    "        journeys = journeys.withColumn('start_station_id', journeys.start_station_id.cast(IntegerType()))\n",
    "        journeys = journeys.withColumn('end_station_id', journeys.end_station_id.cast(IntegerType()))\n",
    "        journeys = journeys.withColumn('start_station_name', journeys.start_station_name.cast(StringType()))\n",
    "        journeys = journeys.withColumn('end_station_name', journeys.end_station_name.cast(StringType()))\n",
    "        journeys = journeys.withColumn('user_type', journeys.user_type.cast(StringType()))\n",
    "        journeys = journeys.withColumn('birth_year', journeys.birth_year.cast(IntegerType()))\n",
    "        #Manual changes\n",
    "        journeys_enriched = journeys.join(matched_pairs_with_pandas, journeys.start_station_name == matched_pairs_with_pandas.station, how='left')\n",
    "        journeys_enriched = journeys_enriched.drop('station')\n",
    "        journeys_enriched = journeys_enriched.join(matched_pairs_with_pandas, journeys_enriched.end_station_name == matched_pairs_with_pandas.station, how='left')\n",
    "        journeys_enriched = journeys_enriched.drop('station')\n",
    "        journeys_enriched = journeys_enriched.withColumnRenamed('neighbourhood', 'start_neighbourhood')\n",
    "        journeys_enriched = journeys_enriched.withColumnRenamed('neighbourhood', 'end_neighbourhood')\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_id', monotonically_increasing_id())\n",
    "        journeys_enriched = journeys_enriched.withColumn('start_date', to_date(journeys_enriched.start_time))\n",
    "        journeys_enriched = journeys_enriched.withColumn('end_date', to_date(journeys_enriched.end_time))\n",
    "        journeys_enriched = journeys_enriched.withColumn('day_of_week', dayofweek(journeys_enriched.start_time))\n",
    "        journeys_enriched = journeys_enriched.withColumn('hour_of_day', hour(journeys_enriched.start_time))\n",
    "        journeys_enriched = journeys_enriched.withColumn('month_of_year', month(journeys_enriched.start_time))\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_duration', (journeys_enriched.end_time.cast(LongType()) - journeys_enriched.start_time.cast(LongType())))\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_duration', journeys_enriched.journey_duration.cast(IntegerType()))\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_duration', journeys_enriched.journey_duration / 60)\n",
    "        # Here, another loop began\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_duration', journeys_enriched.journey_duration.cast(IntegerType()))\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_duration', journeys_enriched.journey_duration / 60)\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_duration', journeys_enriched.journey_duration.cast(IntegerType()))\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_duration', journeys_enriched.journey_duration / 60)\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_duration', journeys_enriched.journey_duration.cast(IntegerType()))\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_duration', journeys_enriched.journey_duration / 60)\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_duration', journeys_enriched.journey_duration.cast(IntegerType()))\n",
    "        self.df_to_db('journeys_enriched', journeys_enriched.toPandas())\n",
    "\n",
    "class BlueBikesDashboard:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        self.engine = create_engine('postgresql://postgres:postgres@localhost:5432/desmondmolloy')\n",
    "        # Create a connection to the engine called `conn`\n",
    "        self.conn = self.engine.connect()\n",
    "    \n",
    "    def create_dash_application(self, response_variable='journeys_count', group_by='start_neighbourhood'):\n",
    "        # Create the Dash app\n",
    "        app = dash.Dash(__name__)\n",
    "        # Create a DataFrame from the Postgres table\n",
    "        df = pd.read_sql('SELECT {}, COUNT(*) as journeys_count FROM journeys_enriched group by 1'.format(group_by), self.conn)\n",
    "        # Create a bar chart of the number of trips by neighbourhood\n",
    "        fig = px.bar(df, x='start_neighbourhood', y='journeys_count')\n",
    "        # Create the Dash app layout\n",
    "        app.layout = html.Div(children=[\n",
    "            html.H1(children='Hello Dash'),\n",
    "            dcc.Graph(\n",
    "                id='example-graph',\n",
    "                figure=fig\n",
    "            )\n",
    "        ])\n",
    "        # Return the app\n",
    "        return app\n",
    "    \n",
    "    def create_dash_application_manual(self, response_variable='journeys_count', group_by='start_neighbourhood'):\n",
    "        # Create the Dash app\n",
    "        app = dash.Dash(__name__)\n",
    "        # Create a DataFrame from the Postgres table\n",
    "        assert response_variable in ['journeys_count', 'journey_duration']\n",
    "        if response_variable == 'journeys_count':\n",
    "            df = pd.read_sql('SELECT {}, COUNT(*) as journeys_count FROM journeys_enriched group by 1'.format(group_by), self.conn)\n",
    "        else:\n",
    "            df = pd.read_sql('SELECT {}, AVG(journey_duration) as journey_duration FROM journeys_enriched group by 1'.format(group_by), self.conn)\n",
    "        df = pd.read_sql('SELECT {}, COUNT(*) as journeys_count FROM journeys_enriched group by 1'.format(group_by), self.conn)\n",
    "        # Create a bar chart of the number of trips by neighbourhood\n",
    "        fig = px.bar(df, x=group_by, y=response_variable)\n",
    "        # Create the Dash app layout\n",
    "        app.layout = html.Div(children=[\n",
    "            html.H1(children='Hello Dash'),\n",
    "            dcc.Graph(\n",
    "                id='example-graph',\n",
    "                figure=fig\n",
    "            )\n",
    "        ])\n",
    "        # Return the app\n",
    "        return app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from bikeshareai.bluebikes import BlueBikesDataPipeline, BlueBikesDashboard\n",
    "# Create an instance of BlueBikesDataPipeline and run the methods\n",
    "pipeline = BlueBikesDataPipeline('https://s3.amazonaws.com/hubway-data/202304-bluebikes-tripdata.zip')\n",
    "pipeline.unzip_file_to_local_csv()\n",
    "pipeline.csv_to_db('journeys', 'data/202304-bluebikes-tripdata.csv')\n",
    "#Arguments were automatically suggested\n",
    "pipeline.main_join('data/Boston_Neighborhoods.geojson', 'data/current_bluebikes_stations.csv', 'data/202304-bluebikes-tripdata.csv')\n",
    "pipeline.enrich_journeys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#        grab_df = joined_df[['Name_left', 'Name_right', 'District']]\n",
    "# Create two classes:\n",
    "# The first one should include the following methods:\n",
    "# __init__: Takes a URL of a zip file as argument, and unzips the file into a CSV in the data folder\n",
    "# connect_to_db: Creates and returns a connection to the local PostgreSQL database using the sqlalchemy package.\n",
    "# df_to_db: Takes the arguments table_name and dataframe. If no table with the name table_name exists in the local PostgreSQL, then insert the csv located at csv_path as a table with the name table_name\n",
    "# main_join: Takes the arguments geojson_path, station_path, and journeys_path, and creates a PySpark context to carry out the following tasks using Spark SQL:\n",
    "# Loads the csv at station_path as a dataframe called stations, and the csv at journeys_path as journeys. Loads the data from geojson path as polydf, loads the table stations as a Geopandas GeoDataFrame with epsg='4326', and then performs a spatial join between polydf and the stations GeoDataFrame. \n",
    "# The resulting Dataframe should be joined to the journey dataframe using journeys.start_station_name = neighbourhood_stations.station twice, producing a table with all of the columns from journeys, plus the start and end neighbourhoods and stations for each journey. Finally, the resulting DataFrame should be written to the PostgreSQL database as a table called journeys_enriched\n",
    "# The second one should include the following methods:\n",
    "# - create_dash_application: create a Dash application. In this application, \n",
    "#it should be possible to select the number of journeys or the average duration as response_variable, and then summarise response_variable by any of the following variables:\n",
    "# start_neighbourhood\n",
    "# end_neighbourhood\n",
    "# day_of_week\n",
    "# hour_of_day\n",
    "# month_of_year\n",
    "# - run: call the create_dash_application method and run the app\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from urllib import request\n",
    "from zipfile import ZipFile\n",
    "from geopandas import GeoDataFrame, read_file, points_from_xy\n",
    "import dash\n",
    "#import dash_core_components as dcc\n",
    "from dash import dcc\n",
    "#import dash_html_components as html\n",
    "from dash import html\n",
    "import plotly.express as px\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType, TimestampType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, dayofweek\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "class BlueBikesDataPipeline:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.engine = create_engine('postgresql://postgres:postgres@localhost:5432/desmondmolloy')\n",
    "        # Create a connection to the engine called `conn`\n",
    "        self.conn = self.engine.connect()\n",
    "\n",
    "    def unzip_file_to_local_csv(self):\n",
    "        # Download the zip file from the URL\n",
    "        request.urlretrieve(self.url, 'data.zip')\n",
    "        # Unzip the file\n",
    "        ZipFile('data.zip').extractall('data')\n",
    "        # Return the unzipped file\n",
    "        # return 'data/tripdata.csv'\n",
    "    \n",
    "    def df_to_db(self, table_name, dataframe):\n",
    "        # Append the data to the `trips` table\n",
    "        dataframe.to_sql(table_name, self.conn, index=False, if_exists='append')\n",
    "\n",
    "\n",
    "    def csv_to_db(self, table_name, csv_path):\n",
    "        # Read in the DataFrame from the CSV file\n",
    "        df = pd.read_csv(csv_path)\n",
    "        # Append the data to the `trips` table\n",
    "        df.to_sql(table_name, self.conn, index=False, if_exists='append')\n",
    "    \n",
    "    def main_join(self, geojson_path, station_path, journeys_path):\n",
    "        poly_df = read_file(geojson_path)\n",
    "        poly_df = poly_df[poly_df['District'] == 'Boston']\n",
    "        poly_df = poly_df[['Name', 'geometry']]\n",
    "        poly_df.columns = ['neighbourhood', 'geometry']\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.lower()\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace(' ', '_')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('-', '_')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('\\'', '')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('(', '')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace(')', '')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('.', '')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('&', 'and')\n",
    "        # Here is where manual was added\n",
    "        stations = pd.read_csv(station_path)\n",
    "        stations = stations[['Station ID', 'Station Name', 'Latitude', 'Longitude']]\n",
    "        stations.columns = ['station_id', 'station_name', 'latitude', 'longitude']\n",
    "        stations['station_name'] = stations['station_name'].str.lower()\n",
    "        stations['station_name'] = stations['station_name'].str.replace(' ', '_')\n",
    "        stations['station_name'] = stations['station_name'].str.replace('-', '_')\n",
    "        stations['station_name'] = stations['station_name'].str.replace('\\'', '')\n",
    "        stations['station_name'] = stations['station_name'].str.replace('(', '')\n",
    "        stations['station_name'] = stations['station_name'].str.replace(')', '')\n",
    "        stations['station_name'] = stations['station_name'].str.replace('.', '')\n",
    "        stations['station_name'] = stations['station_name'].str.replace('&', 'and')\n",
    "        # Here, the loop began again, forcing manual changes\n",
    "        stations_geo = GeoDataFrame(stations, geometry=points_from_xy(stations.longitude, stations.latitude))\n",
    "        stations_geo.set_crs(epsg='4326', inplace=True)\n",
    "        joined_df = stations_geo.sjoin(poly_df, how=\"left\")\n",
    "        grab_df = joined_df[['Name_left', 'Name_right', 'District']]\n",
    "        matched_pairs_with_pandas = grab_df[grab_df['District'] == 'Boston']\n",
    "        matched_pairs_with_pandas.columns = ['station', 'neighbourhood']\n",
    "        spark = SparkSession.builder.appName(\"BlueBikes\").getOrCreate()\n",
    "        schema = StructType([])\n",
    "        journeys = spark.read.csv(journeys_path, header=True, schema=schema)\n",
    "        journeys = journeys.withColumnRenamed('starttime', 'start_time')\n",
    "        journeys = journeys.withColumnRenamed('stoptime', 'end_time')\n",
    "        journeys = journeys.withColumnRenamed('start station id', 'start_station_id')\n",
    "        journeys = journeys.withColumnRenamed('end station id', 'end_station_id')\n",
    "        journeys = journeys.withColumnRenamed('start station name', 'start_station_name')\n",
    "        journeys = journeys.withColumnRenamed('end station name', 'end_station_name')\n",
    "        journeys = journeys.withColumnRenamed('usertype', 'user_type')\n",
    "        journeys = journeys.withColumnRenamed('birth year', 'birth_year')\n",
    "        #Suugestions died here\n",
    "        journeys = journeys.withColumn('start_time', to_timestamp(journeys.start_time, 'MM/dd/yyyy HH:mm'))\n",
    "        journeys = journeys.withColumn('end_time', to_timestamp(journeys.end_time, 'MM/dd/yyyy HH:mm'))\n",
    "        journeys = journeys.withColumn('start_station_id', journeys.start_station_id.cast(IntegerType()))\n",
    "        journeys = journeys.withColumn('end_station_id', journeys.end_station_id.cast(IntegerType()))\n",
    "        journeys = journeys.withColumn('start_station_name', journeys.start_station_name.cast(StringType()))\n",
    "        journeys = journeys.withColumn('end_station_name', journeys.end_station_name.cast(StringType()))\n",
    "        journeys = journeys.withColumn('user_type', journeys.user_type.cast(StringType()))\n",
    "        journeys = journeys.withColumn('birth_year', journeys.birth_year.cast(IntegerType()))\n",
    "        #Manual changes\n",
    "        journeys_enriched = journeys.join(matched_pairs_with_pandas, journeys.start_station_name == matched_pairs_with_pandas.station, how='left')\n",
    "        journeys_enriched = journeys_enriched.drop('station')\n",
    "        journeys_enriched = journeys_enriched.join(matched_pairs_with_pandas, journeys_enriched.end_station_name == matched_pairs_with_pandas.station, how='left')\n",
    "        journeys_enriched = journeys_enriched.drop('station')\n",
    "        journeys_enriched = journeys_enriched.withColumnRenamed('neighbourhood', 'start_neighbourhood')\n",
    "        journeys_enriched = journeys_enriched.withColumnRenamed('neighbourhood', 'end_neighbourhood')\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_id', monotonically_increasing_id())\n",
    "        journeys_enriched = journeys_enriched.withColumn('start_date', to_date(journeys_enriched.start_time))\n",
    "        journeys_enriched = journeys_enriched.withColumn('end_date', to_date(journeys_enriched.end_time))\n",
    "        journeys_enriched = journeys_enriched.withColumn('day_of_week', dayofweek(journeys_enriched.start_time))\n",
    "        journeys_enriched = journeys_enriched.withColumn('hour_of_day', hour(journeys_enriched.start_time))\n",
    "        journeys_enriched = journeys_enriched.withColumn('month_of_year', month(journeys_enriched.start_time))\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_duration', (journeys_enriched.end_time.cast(LongType()) - journeys_enriched.start_time.cast(LongType())))\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_duration', journeys_enriched.journey_duration.cast(IntegerType()))\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_duration', journeys_enriched.journey_duration / 60)\n",
    "        # Here, another loop began\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_duration', journeys_enriched.journey_duration.cast(IntegerType()))\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_duration', journeys_enriched.journey_duration / 60)\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_duration', journeys_enriched.journey_duration.cast(IntegerType()))\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_duration', journeys_enriched.journey_duration / 60)\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_duration', journeys_enriched.journey_duration.cast(IntegerType()))\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_duration', journeys_enriched.journey_duration / 60)\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_duration', journeys_enriched.journey_duration.cast(IntegerType()))\n",
    "        self.df_to_db('journeys_enriched', journeys_enriched.toPandas())\n",
    "\n",
    "class BlueBikesDashboard:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        self.engine = create_engine('postgresql://postgres:postgres@localhost:5432/desmondmolloy')\n",
    "        # Create a connection to the engine called `conn`\n",
    "        self.conn = self.engine.connect()\n",
    "    \n",
    "    def create_dash_application_manual(self, response_variable='journeys_count', group_by='start_neighbourhood'):\n",
    "        # Create the Dash app\n",
    "        app = dash.Dash(__name__)\n",
    "        # Create a DataFrame from the Postgres table\n",
    "        assert response_variable in ['journeys_count', 'journey_duration']\n",
    "        if response_variable == 'journeys_count':\n",
    "            df = pd.read_sql('SELECT {}, COUNT(*) as journeys_count FROM journeys_enriched group by 1'.format(group_by), self.conn)\n",
    "        else:\n",
    "            df = pd.read_sql('SELECT {}, AVG(journey_duration) as journey_duration FROM journeys_enriched group by 1'.format(group_by), self.conn)\n",
    "        df = pd.read_sql('SELECT {}, COUNT(*) as journeys_count FROM journeys_enriched group by 1'.format(group_by), self.conn)\n",
    "        # Create a bar chart of the number of trips by neighbourhood\n",
    "        fig = px.bar(df, x=group_by, y=response_variable)\n",
    "        # Create the Dash app layout\n",
    "        app.layout = html.Div(children=[\n",
    "            html.H1(children='Hello Dash'),\n",
    "            dcc.Graph(\n",
    "                id='example-graph',\n",
    "                figure=fig\n",
    "            )\n",
    "        ])\n",
    "        # Return the app\n",
    "        return app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'District'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/GitHub/BikeShareAI/.conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Documents/GitHub/BikeShareAI/.conda/lib/python3.11/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/GitHub/BikeShareAI/.conda/lib/python3.11/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'District'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m pipeline\u001b[39m.\u001b[39mcsv_to_db(\u001b[39m'\u001b[39m\u001b[39mjourneys\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdata/202304-bluebikes-tripdata.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39m#Arguments were automatically suggested\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m pipeline\u001b[39m.\u001b[39;49mmain_join(\u001b[39m'\u001b[39;49m\u001b[39mdata/Boston_Neighborhoods.geojson\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mdata/current_bluebikes_stations.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mdata/202304-bluebikes-tripdata.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      8\u001b[0m pipeline\u001b[39m.\u001b[39menrich_journeys()\n",
      "Cell \u001b[0;32mIn[7], line 70\u001b[0m, in \u001b[0;36mBlueBikesDataPipeline.main_join\u001b[0;34m(self, geojson_path, station_path, journeys_path)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain_join\u001b[39m(\u001b[39mself\u001b[39m, geojson_path, station_path, journeys_path):\n\u001b[1;32m     69\u001b[0m     poly_df \u001b[39m=\u001b[39m read_file(geojson_path)\n\u001b[0;32m---> 70\u001b[0m     poly_df \u001b[39m=\u001b[39m poly_df[poly_df[\u001b[39m'\u001b[39;49m\u001b[39mDistrict\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mBoston\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     71\u001b[0m     poly_df \u001b[39m=\u001b[39m poly_df[[\u001b[39m'\u001b[39m\u001b[39mName\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgeometry\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m     72\u001b[0m     poly_df\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mneighbourhood\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgeometry\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/GitHub/BikeShareAI/.conda/lib/python3.11/site-packages/geopandas/geodataframe.py:1475\u001b[0m, in \u001b[0;36mGeoDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1469\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[1;32m   1470\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1471\u001b[0m \u001b[39m    If the result is a column containing only 'geometry', return a\u001b[39;00m\n\u001b[1;32m   1472\u001b[0m \u001b[39m    GeoSeries. If it's a DataFrame with any columns of GeometryDtype,\u001b[39;00m\n\u001b[1;32m   1473\u001b[0m \u001b[39m    return a GeoDataFrame.\u001b[39;00m\n\u001b[1;32m   1474\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1475\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(key)\n\u001b[1;32m   1476\u001b[0m     \u001b[39m# Custom logic to avoid waiting for pandas GH51895\u001b[39;00m\n\u001b[1;32m   1477\u001b[0m     \u001b[39m# result is not geometry dtype for multi-indexes\u001b[39;00m\n\u001b[1;32m   1478\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1479\u001b[0m         pd\u001b[39m.\u001b[39mapi\u001b[39m.\u001b[39mtypes\u001b[39m.\u001b[39mis_scalar(key)\n\u001b[1;32m   1480\u001b[0m         \u001b[39mand\u001b[39;00m key \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1483\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_geometry_type(result)\n\u001b[1;32m   1484\u001b[0m     ):\n",
      "File \u001b[0;32m~/Documents/GitHub/BikeShareAI/.conda/lib/python3.11/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Documents/GitHub/BikeShareAI/.conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'District'"
     ]
    }
   ],
   "source": [
    "#from bikeshareai.bluebikes import BlueBikesDataPipeline, BlueBikesDashboard\n",
    "# Create an instance of BlueBikesDataPipeline and run the methods\n",
    "pipeline = BlueBikesDataPipeline('https://s3.amazonaws.com/hubway-data/202304-bluebikes-tripdata.zip')\n",
    "pipeline.unzip_file_to_local_csv()\n",
    "pipeline.csv_to_db('journeys', 'data/202304-bluebikes-tripdata.csv')\n",
    "#Arguments were automatically suggested\n",
    "pipeline.main_join('data/Boston_Neighborhoods.geojson', 'data/current_bluebikes_stations.csv', 'data/202304-bluebikes-tripdata.csv')\n",
    "pipeline.enrich_journeys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df = joined_df[['Name_left', 'Name_right', 'District']]\n",
    "        matched_pairs_with_pandas = grab_df[grab_df['District'] == 'Boston']\n",
    "        matched_pairs_with_pandas.columns = ['station', 'neighbourhood', 'District']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#        grab_df = joined_df[['Name_left', 'Name_right', 'District']]\n",
    "# Create two classes:\n",
    "# The first one should include the following methods:\n",
    "# __init__: Takes a URL of a zip file as argument, and unzips the file into a CSV in the data folder\n",
    "# connect_to_db: Creates and returns a connection to the local PostgreSQL database using the sqlalchemy package.\n",
    "# df_to_db: Takes the arguments table_name and dataframe. If no table with the name table_name exists in the local PostgreSQL, then insert the csv located at csv_path as a table with the name table_name\n",
    "# main_join: Takes the arguments geojson_path, station_path, and journeys_path, and creates a PySpark context to carry out the following tasks using Spark SQL:\n",
    "# Loads the csv at station_path as a dataframe called stations, and the csv at journeys_path as journeys. Loads the data from geojson path as polydf, loads the table stations as a Geopandas GeoDataFrame with epsg='4326', and then performs a spatial join between polydf and the stations GeoDataFrame. \n",
    "# The resulting Dataframe should be joined to the journey dataframe using journeys.start_station_name = neighbourhood_stations.station twice, producing a table with all of the columns from journeys, plus the start and end neighbourhoods and stations for each journey. Finally, the resulting DataFrame should be written to the PostgreSQL database as a table called journeys_enriched\n",
    "# The second one should include the following methods:\n",
    "# - create_dash_application: create a Dash application. In this application, \n",
    "#it should be possible to select the number of journeys or the average duration as response_variable, and then summarise response_variable by any of the following variables:\n",
    "# start_neighbourhood\n",
    "# end_neighbourhood\n",
    "# day_of_week\n",
    "# hour_of_day\n",
    "# month_of_year\n",
    "# - run: call the create_dash_application method and run the app\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from urllib import request\n",
    "from zipfile import ZipFile\n",
    "from geopandas import GeoDataFrame, read_file, points_from_xy\n",
    "import dash\n",
    "#import dash_core_components as dcc\n",
    "from dash import dcc\n",
    "#import dash_html_components as html\n",
    "from dash import html\n",
    "import plotly.express as px\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType, TimestampType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, dayofweek\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "class BlueBikesDataPipeline:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.engine = create_engine('postgresql://postgres:postgres@localhost:5432/desmondmolloy')\n",
    "        # Create a connection to the engine called `conn`\n",
    "        self.conn = self.engine.connect()\n",
    "\n",
    "    def unzip_file_to_local_csv(self):\n",
    "        # Download the zip file from the URL\n",
    "        request.urlretrieve(self.url, 'data.zip')\n",
    "        # Unzip the file\n",
    "        ZipFile('data.zip').extractall('data')\n",
    "        # Return the unzipped file\n",
    "        # return 'data/tripdata.csv'\n",
    "    \n",
    "    def df_to_db(self, table_name, dataframe):\n",
    "        # Append the data to the `trips` table\n",
    "        dataframe.to_sql(table_name, self.conn, index=False, if_exists='append')\n",
    "\n",
    "\n",
    "    def csv_to_db(self, table_name, csv_path):\n",
    "        # Read in the DataFrame from the CSV file\n",
    "        df = pd.read_csv(csv_path)\n",
    "        # Append the data to the `trips` table\n",
    "        df.to_sql(table_name, self.conn, index=False, if_exists='append')\n",
    "    \n",
    "    def main_join(self, geojson_path, station_path, journeys_path):\n",
    "        poly_df = read_file(geojson_path)\n",
    "#        poly_df = poly_df[poly_df['District'] == 'Boston']\n",
    "        poly_df = poly_df[['Name', 'geometry']]\n",
    "        poly_df.columns = ['neighbourhood', 'geometry']\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.lower()\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace(' ', '_')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('-', '_')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('\\'', '')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('(', '')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace(')', '')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('.', '')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('&', 'and')\n",
    "        # Here is where manual was added\n",
    "        stations = pd.read_csv(station_path)\n",
    "        #stations = stations[['Station ID', 'Station Name', 'Latitude', 'Longitude']]\n",
    "        stations = stations[['Number', 'Name', 'Latitude', 'Longitude']]\n",
    "        stations.columns = ['station_id', 'station_name', 'latitude', 'longitude']\n",
    "        stations['station_name'] = stations['station_name'].str.lower()\n",
    "        stations['station_name'] = stations['station_name'].str.replace(' ', '_')\n",
    "        stations['station_name'] = stations['station_name'].str.replace('-', '_')\n",
    "        stations['station_name'] = stations['station_name'].str.replace('\\'', '')\n",
    "        stations['station_name'] = stations['station_name'].str.replace('(', '')\n",
    "        stations['station_name'] = stations['station_name'].str.replace(')', '')\n",
    "        stations['station_name'] = stations['station_name'].str.replace('.', '')\n",
    "        stations['station_name'] = stations['station_name'].str.replace('&', 'and')\n",
    "        # Here, the loop began again, forcing manual changes\n",
    "        stations_geo = GeoDataFrame(stations, geometry=points_from_xy(stations.longitude, stations.latitude))\n",
    "        stations_geo.set_crs(epsg='4326', inplace=True)\n",
    "        joined_df = stations_geo.sjoin(poly_df, how=\"left\")\n",
    "        spark = SparkSession.builder.appName(\"BlueBikes\").getOrCreate()\n",
    "        schema = StructType([])\n",
    "        journeys = spark.read.csv(journeys_path, header=True, schema=schema)\n",
    "        #started_at was actual name\n",
    "        #journeys = journeys.withColumnRenamed('started_at', 'start_time')\n",
    "# This line was automatic\n",
    "        #journeys = journeys.withColumnRenamed('ended_at', 'end_time')\n",
    "\n",
    "        #Suugestions died here\n",
    "        journeys = journeys.withColumn('start_time', to_timestamp(journeys.started_at, 'MM/dd/yyyy HH:mm'))\n",
    "        journeys = journeys.withColumn('end_time', to_timestamp(journeys.ended_at, 'MM/dd/yyyy HH:mm'))\n",
    "        journeys = journeys.withColumn('start_station_id', journeys.start_station_id.cast(IntegerType()))\n",
    "        journeys = journeys.withColumn('end_station_id', journeys.end_station_id.cast(IntegerType()))\n",
    "        journeys = journeys.withColumn('start_station_name', journeys.start_station_name.cast(StringType()))\n",
    "        journeys = journeys.withColumn('end_station_name', journeys.end_station_name.cast(StringType()))\n",
    "        #Manual changes\n",
    "        journeys_enriched = journeys.join(grab_df, journeys.start_station_name == grab_df.station, how='left')\n",
    "        journeys_enriched = journeys_enriched.drop('station')\n",
    "        journeys_enriched = journeys_enriched.join(grab_df, journeys_enriched.end_station_name == grab_df.station, how='left')\n",
    "        journeys_enriched = journeys_enriched.drop('station')\n",
    "        journeys_enriched = journeys_enriched.withColumnRenamed('neighbourhood', 'start_neighbourhood')\n",
    "        journeys_enriched = journeys_enriched.withColumnRenamed('neighbourhood', 'end_neighbourhood')\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_id', monotonically_increasing_id())\n",
    "        journeys_enriched = journeys_enriched.withColumn('start_date', to_date(journeys_enriched.start_time))\n",
    "        journeys_enriched = journeys_enriched.withColumn('end_date', to_date(journeys_enriched.end_time))\n",
    "        journeys_enriched = journeys_enriched.withColumn('day_of_week', dayofweek(journeys_enriched.start_time))\n",
    "        journeys_enriched = journeys_enriched.withColumn('hour_of_day', hour(journeys_enriched.start_time))\n",
    "        journeys_enriched = journeys_enriched.withColumn('month_of_year', month(journeys_enriched.start_time))\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_duration', (journeys_enriched.end_time.cast(LongType()) - journeys_enriched.start_time.cast(LongType())))\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_duration', journeys_enriched.journey_duration.cast(IntegerType()))\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_duration', journeys_enriched.journey_duration / 60)\n",
    "        # Here, another loop began\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_duration', journeys_enriched.journey_duration.cast(IntegerType()))\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_duration', journeys_enriched.journey_duration / 60)\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_duration', journeys_enriched.journey_duration.cast(IntegerType()))\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_duration', journeys_enriched.journey_duration / 60)\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_duration', journeys_enriched.journey_duration.cast(IntegerType()))\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_duration', journeys_enriched.journey_duration / 60)\n",
    "        journeys_enriched = journeys_enriched.withColumn('journey_duration', journeys_enriched.journey_duration.cast(IntegerType()))\n",
    "        self.df_to_db('journeys_enriched', journeys_enriched.toPandas())\n",
    "\n",
    "class BlueBikesDashboard:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        self.engine = create_engine('postgresql://postgres:postgres@localhost:5432/desmondmolloy')\n",
    "        # Create a connection to the engine called `conn`\n",
    "        self.conn = self.engine.connect()\n",
    "    \n",
    "    def create_dash_application_manual(self, response_variable='journeys_count', group_by='start_neighbourhood'):\n",
    "        # Create the Dash app\n",
    "        app = dash.Dash(__name__)\n",
    "        # Create a DataFrame from the Postgres table\n",
    "        assert response_variable in ['journeys_count', 'journey_duration']\n",
    "        if response_variable == 'journeys_count':\n",
    "            df = pd.read_sql('SELECT {}, COUNT(*) as journeys_count FROM journeys_enriched group by 1'.format(group_by), self.conn)\n",
    "        else:\n",
    "            df = pd.read_sql('SELECT {}, AVG(journey_duration) as journey_duration FROM journeys_enriched group by 1'.format(group_by), self.conn)\n",
    "        df = pd.read_sql('SELECT {}, COUNT(*) as journeys_count FROM journeys_enriched group by 1'.format(group_by), self.conn)\n",
    "        # Create a bar chart of the number of trips by neighbourhood\n",
    "        fig = px.bar(df, x=group_by, y=response_variable)\n",
    "        # Create the Dash app layout\n",
    "        app.layout = html.Div(children=[\n",
    "            html.H1(children='Hello Dash'),\n",
    "            dcc.Graph(\n",
    "                id='example-graph',\n",
    "                figure=fig\n",
    "            )\n",
    "        ])\n",
    "        # Return the app\n",
    "        return app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/7wfwn_sn3cv5zpj5dk7k9r3r0000gn/T/ipykernel_1794/399321039.py:77: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('(', '')\n",
      "/var/folders/63/7wfwn_sn3cv5zpj5dk7k9r3r0000gn/T/ipykernel_1794/399321039.py:78: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace(')', '')\n",
      "/var/folders/63/7wfwn_sn3cv5zpj5dk7k9r3r0000gn/T/ipykernel_1794/399321039.py:79: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('.', '')\n",
      "/var/folders/63/7wfwn_sn3cv5zpj5dk7k9r3r0000gn/T/ipykernel_1794/399321039.py:90: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  stations['station_name'] = stations['station_name'].str.replace('(', '')\n",
      "/var/folders/63/7wfwn_sn3cv5zpj5dk7k9r3r0000gn/T/ipykernel_1794/399321039.py:91: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  stations['station_name'] = stations['station_name'].str.replace(')', '')\n",
      "/var/folders/63/7wfwn_sn3cv5zpj5dk7k9r3r0000gn/T/ipykernel_1794/399321039.py:92: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  stations['station_name'] = stations['station_name'].str.replace('.', '')\n",
      "23/05/21 18:30:46 WARN Utils: Your hostname, Desmonds-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.3 instead (on interface en0)\n",
      "23/05/21 18:30:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/21 18:30:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'started_at'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m pipeline\u001b[39m.\u001b[39mcsv_to_db(\u001b[39m'\u001b[39m\u001b[39mjourneys\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdata/202304-bluebikes-tripdata.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39m#Arguments were automatically suggested\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m pipeline\u001b[39m.\u001b[39;49mmain_join(\u001b[39m'\u001b[39;49m\u001b[39mdata/Boston_Neighborhoods.geojson\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mdata/current_bluebikes_stations.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mdata/202304-bluebikes-tripdata.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      8\u001b[0m pipeline\u001b[39m.\u001b[39menrich_journeys()\n",
      "Cell \u001b[0;32mIn[1], line 107\u001b[0m, in \u001b[0;36mBlueBikesDataPipeline.main_join\u001b[0;34m(self, geojson_path, station_path, journeys_path)\u001b[0m\n\u001b[1;32m    100\u001b[0m         journeys \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39mcsv(journeys_path, header\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, schema\u001b[39m=\u001b[39mschema)\n\u001b[1;32m    101\u001b[0m         \u001b[39m#started_at was actual name\u001b[39;00m\n\u001b[1;32m    102\u001b[0m         \u001b[39m#journeys = journeys.withColumnRenamed('started_at', 'start_time')\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39m# This line was automatic\u001b[39;00m\n\u001b[1;32m    104\u001b[0m         \u001b[39m#journeys = journeys.withColumnRenamed('ended_at', 'end_time')\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m         \u001b[39m#Suugestions died here\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m         journeys \u001b[39m=\u001b[39m journeys\u001b[39m.\u001b[39mwithColumn(\u001b[39m'\u001b[39m\u001b[39mstart_time\u001b[39m\u001b[39m'\u001b[39m, to_timestamp(journeys\u001b[39m.\u001b[39;49mstarted_at, \u001b[39m'\u001b[39m\u001b[39mMM/dd/yyyy HH:mm\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m    108\u001b[0m         journeys \u001b[39m=\u001b[39m journeys\u001b[39m.\u001b[39mwithColumn(\u001b[39m'\u001b[39m\u001b[39mend_time\u001b[39m\u001b[39m'\u001b[39m, to_timestamp(journeys\u001b[39m.\u001b[39mended_at, \u001b[39m'\u001b[39m\u001b[39mMM/dd/yyyy HH:mm\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m    109\u001b[0m         journeys \u001b[39m=\u001b[39m journeys\u001b[39m.\u001b[39mwithColumn(\u001b[39m'\u001b[39m\u001b[39mstart_station_id\u001b[39m\u001b[39m'\u001b[39m, journeys\u001b[39m.\u001b[39mstart_station_id\u001b[39m.\u001b[39mcast(IntegerType()))\n",
      "File \u001b[0;32m~/Documents/GitHub/BikeShareAI/.conda/lib/python3.11/site-packages/pyspark/sql/dataframe.py:2977\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2944\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   2945\u001b[0m \n\u001b[1;32m   2946\u001b[0m \u001b[39m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2974\u001b[0m \u001b[39m+---+\u001b[39;00m\n\u001b[1;32m   2975\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2976\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns:\n\u001b[0;32m-> 2977\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m   2978\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name)\n\u001b[1;32m   2979\u001b[0m     )\n\u001b[1;32m   2980\u001b[0m jc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jdf\u001b[39m.\u001b[39mapply(name)\n\u001b[1;32m   2981\u001b[0m \u001b[39mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'started_at'"
     ]
    }
   ],
   "source": [
    "#from bikeshareai.bluebikes import BlueBikesDataPipeline, BlueBikesDashboard\n",
    "# Create an instance of BlueBikesDataPipeline and run the methods\n",
    "pipeline = BlueBikesDataPipeline('https://s3.amazonaws.com/hubway-data/202304-bluebikes-tripdata.zip')\n",
    "pipeline.unzip_file_to_local_csv()\n",
    "#Arguments were automatically suggested\n",
    "pipeline.main_join('data/Boston_Neighborhoods.geojson', 'data/current_bluebikes_stations.csv', 'data/202304-bluebikes-tripdata.csv')\n",
    "pipeline.enrich_journeys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/21 18:34:50 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 13, schema size: 0\n",
      "CSV file: file:///Users/desmondmolloy/Documents/GitHub/BikeShareAI/data/202304-bluebikes-tripdata.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"BlueBikes\").getOrCreate()\n",
    "schema = StructType([])\n",
    "journeys = spark.read.csv('data/202304-bluebikes-tripdata.csv', header=True, schema=schema)\n",
    "journeys_pandas = journeys.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "journeys_pandas.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark was dropped as an option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two classes:\n",
    "# The first one should include the following methods:\n",
    "# __init__: Takes a URL of a zip file as argument, and unzips the file into a CSV in the data folder\n",
    "# connect_to_db: Creates and returns a connection to the local PostgreSQL database using the sqlalchemy package.\n",
    "# df_to_db: Takes the arguments table_name and dataframe. If no table with the name table_name exists in the local PostgreSQL, then insert the csv located at csv_path as a table with the name table_name\n",
    "# main_join: Takes the arguments geojson_path, station_path, and journeys_path. Loads the csv at station_path as a dataframe called stations, and the csv at journeys_path as journeys. Loads the data from geojson path as polydf, loads the table stations as a Geopandas GeoDataFrame with epsg='4326', and then performs a spatial join between polydf and the stations GeoDataFrame. \n",
    "# The resulting Dataframe should be joined to the journey dataframe using journeys.start_station_name = neighbourhood_stations.station twice, producing a table with all of the columns from journeys, plus the start and end neighbourhoods and stations for each journey. Finally, the resulting DataFrame should be written to the PostgreSQL database as a table called journeys_enriched\n",
    "# The second one should include the following methods:\n",
    "# - create_dash_application: create a Dash application. In this application, \n",
    "#it should be possible to select the number of journeys or the average duration as response_variable, and then summarise response_variable by any of the following variables:\n",
    "# start_neighbourhood\n",
    "# end_neighbourhood\n",
    "# day_of_week\n",
    "# hour_of_day\n",
    "# month_of_year\n",
    "# - run: call the create_dash_application method and run the app\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from urllib import request\n",
    "from zipfile import ZipFile\n",
    "from geopandas import GeoDataFrame, read_file, points_from_xy\n",
    "import dash\n",
    "#import dash_core_components as dcc\n",
    "from dash import dcc\n",
    "#import dash_html_components as html\n",
    "from dash import html\n",
    "import plotly.express as px\n",
    "\n",
    "class BlueBikesDataPipeline:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.engine = create_engine('postgresql://postgres:postgres@localhost:5432/desmondmolloy')\n",
    "        # Create a connection to the engine called `conn`\n",
    "        self.conn = self.engine.connect()\n",
    "    \n",
    "    def unzip_file_to_local_csv(self):\n",
    "        # Download the zip file from the URL\n",
    "        request.urlretrieve(self.url, 'data.zip')\n",
    "        # Unzip the file\n",
    "        ZipFile('data.zip').extractall('data')\n",
    "        # Return the unzipped file\n",
    "        # return 'data/tripdata.csv'\n",
    "    \n",
    "    def df_to_db(self, table_name, dataframe):\n",
    "        # Append the data to the `trips` table\n",
    "        dataframe.to_sql(table_name, self.conn, index=False, if_exists='append')\n",
    "    \n",
    "    def main_join(self, geojson_path, station_path, journeys_path):\n",
    "        poly_df = read_file(geojson_path)\n",
    "#        poly_df = poly_df[poly_df['District'] == 'Boston']\n",
    "        poly_df = poly_df[['Name', 'geometry']]\n",
    "        poly_df.columns = ['neighbourhood', 'geometry']\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.lower()\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace(' ', '_')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('-', '_')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('\\'', '')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('(', '')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace(')', '')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('.', '')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('&', 'and')\n",
    "        # Here is where manual was added\n",
    "        stations = pd.read_csv(station_path)\n",
    "        #stations = stations[['Station ID', 'Station Name', 'Latitude', 'Longitude']]\n",
    "        stations = stations[['Number', 'Name', 'Latitude', 'Longitude']]\n",
    "        stations.columns = ['station_id', 'station_name', 'latitude', 'longitude']\n",
    "        stations['station_name'] = stations['station_name'].str.lower()\n",
    "        stations['station_name'] = stations['station_name'].str.replace(' ', '_')\n",
    "        stations['station_name'] = stations['station_name'].str.replace('-', '_')\n",
    "        stations['station_name'] = stations['station_name'].str.replace('\\'', '')\n",
    "        stations['station_name'] = stations['station_name'].str.replace('(', '')\n",
    "        stations['station_name'] = stations['station_name'].str.replace(')', '')\n",
    "        stations['station_name'] = stations['station_name'].str.replace('.', '')\n",
    "        stations['station_name'] = stations['station_name'].str.replace('&', 'and')\n",
    "        # Here, the loop began again, forcing manual changes\n",
    "        stations_geo = GeoDataFrame(stations, geometry=points_from_xy(stations.longitude, stations.latitude))\n",
    "        stations_geo.set_crs(epsg='4326', inplace=True)\n",
    "        joined_df = stations_geo.sjoin(poly_df, how=\"left\")\n",
    "        #grab_df = joined_df[['Name_left', 'Name_right', 'District']]\n",
    "        #matched_pairs_with_pandas = grab_df[grab_df['District'] == 'Boston']\n",
    "        #matched_pairs_with_pandas.columns = ['station', 'neighbourhood', 'District']\n",
    "        # Create a SparkSession\n",
    "        spark = SparkSession.builder.appName(\"BlueBikes\").getOrCreate()\n",
    "        # Create a schema for the dataframe\n",
    "        schema = StructType([])\n",
    "        # Load the CSV file into a dataframe\n",
    "        #journeys = spark.read.csv(journeys_path, header=True, schema=schema)\n",
    "        journeys_pandas_df = pd.read_csv(journeys_path)\n",
    "                "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more specific version of the visualisation class prompt (describing specific SQL steps):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two classes:\n",
    "# The first one should include the following methods:\n",
    "# __init__: Takes a URL of a zip file as argument, and unzips the file into a CSV in the data folder\n",
    "# connect_to_db: Creates and returns a connection to the local PostgreSQL database using the sqlalchemy package.\n",
    "# df_to_db: Takes the arguments table_name and dataframe. If no table with the name table_name exists in the local PostgreSQL, then insert the csv located at csv_path as a table with the name table_name\n",
    "# main_join: Takes the arguments geojson_path, station_path, and journeys_path. Loads the csv at station_path as a dataframe called stations, and the csv at journeys_path as journeys. Loads the data from geojson path as polydf, loads the table stations as a Geopandas GeoDataFrame with epsg='4326', and then performs a spatial join between polydf and the stations GeoDataFrame. \n",
    "# The resulting Dataframe should be joined to the journey dataframe using journeys.start_station_name = neighbourhood_stations.station twice, producing a table with all of the columns from journeys, plus the start and end neighbourhoods and stations for each journey. Finally, the resulting DataFrame should be written to the PostgreSQL database as a table called journeys_enriched\n",
    "# the second one should be called BlueBikesVisual and have the following methods\n",
    "# - __init__: Connect to the local PostgreSQL instance\n",
    "# - create_dash_application: create a Dash application. It should have two dropdown menus, with the variables response_variable and grouping_variable\n",
    "# response_variable should be in [\"journeys\",\"duration\"]. If response_variable = journeys, then pass a SQL statement that counts all records by grouping_variable\n",
    "# If response_variable is duration, then pass a SQL statement that selects the mean duration froup by grouping variable\n",
    "# Grouping variable should be in the following:\n",
    "# start_neighbourhood\n",
    "# end_neighbourhood\n",
    "# day_of_week\n",
    "# hour_of_day\n",
    "# month_of_year\n",
    "# The application should have a bar graph showing the response variable by grouping variable\n",
    "# - run: call the create_dash_application method and run the app\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from urllib import request\n",
    "from zipfile import ZipFile\n",
    "from geopandas import GeoDataFrame, read_file, points_from_xy\n",
    "import dash\n",
    "#import dash_core_components as dcc\n",
    "from dash import dcc\n",
    "#import dash_html_components as html\n",
    "from dash import html\n",
    "import plotly.express as px\n",
    "\n",
    "class BlueBikesDataPipeline:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.engine = create_engine('postgresql://postgres:postgres@localhost:5432/desmondmolloy')\n",
    "        # Create a connection to the engine called `conn`\n",
    "        self.conn = self.engine.connect()\n",
    "    \n",
    "    def unzip_file_to_local_csv(self):\n",
    "        # Download the zip file from the URL\n",
    "        request.urlretrieve(self.url, 'data.zip')\n",
    "        # Unzip the file\n",
    "        ZipFile('data.zip').extractall('data')\n",
    "        # Return the unzipped file\n",
    "        # return 'data/tripdata.csv'\n",
    "    \n",
    "    def df_to_db(self, table_name, dataframe):\n",
    "        # Append the data to the `trips` table\n",
    "        dataframe.to_sql(table_name, self.conn, index=False, if_exists='append')\n",
    "    \n",
    "    def main_join(self, geojson_path, station_path, journeys_path):\n",
    "        poly_df = read_file(geojson_path)\n",
    "#        poly_df = poly_df[poly_df['District'] == 'Boston']\n",
    "        poly_df = poly_df[['Name', 'geometry']]\n",
    "        poly_df.columns = ['neighbourhood', 'geometry']\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.lower()\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace(' ', '_')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('-', '_')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('\\'', '')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('(', '')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace(')', '')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('.', '')\n",
    "        poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('&', 'and')\n",
    "        # Here is where manual was added\n",
    "        stations = pd.read_csv(station_path)\n",
    "        #stations = stations[['Station ID', 'Station Name', 'Latitude', 'Longitude']]\n",
    "        stations = stations[['Number', 'Name', 'Latitude', 'Longitude']]\n",
    "        stations.columns = ['station_id', 'station_name', 'latitude', 'longitude']\n",
    "        stations['station_name'] = stations['station_name'].str.lower()\n",
    "        stations['station_name'] = stations['station_name'].str.replace(' ', '_')\n",
    "        stations['station_name'] = stations['station_name'].str.replace('-', '_')\n",
    "        stations['station_name'] = stations['station_name'].str.replace('\\'', '')\n",
    "        stations['station_name'] = stations['station_name'].str.replace('(', '')\n",
    "        stations['station_name'] = stations['station_name'].str.replace(')', '')\n",
    "        stations['station_name'] = stations['station_name'].str.replace('.', '')\n",
    "        stations['station_name'] = stations['station_name'].str.replace('&', 'and')\n",
    "        # Here, the loop began again, forcing manual changes\n",
    "        stations_geo = GeoDataFrame(stations, geometry=points_from_xy(stations.longitude, stations.latitude))\n",
    "        stations_geo.set_crs(epsg='4326', inplace=True)\n",
    "        joined_df = stations_geo.sjoin(poly_df, how=\"left\")\n",
    "        journeys_pandas_df = pd.read_csv(journeys_path)\n",
    "        journeys_enriched = journeys_pandas_df.merge(joined_df, left_on='start_station_name', right_on='station_name', how='left')\n",
    "        journeys_enriched = journeys_enriched.merge(joined_df, left_on='end_station_name', right_on='station_name', how='left')\n",
    "        journeys_enriched = journeys_enriched.drop(['station_name_x', 'station_name_y'], axis=1)\n",
    "        journeys_enriched = journeys_enriched.rename(columns={'Name_x': 'start_neighbourhood', 'Name_y': 'end_neighbourhood'})\n",
    "        journeys_enriched = journeys_enriched.dropna()\n",
    "        journeys_enriched['duration'] = pd.to_datetime(journeys_enriched['ended_at']) - pd.to_datetime(journeys_enriched['started_at'])\n",
    "        journeys_enriched['duration'] = journeys_enriched['duration'].dt.total_seconds()\n",
    "        journeys_enriched['duration'] = journeys_enriched['duration'] / 60\n",
    "        journeys_enriched['duration'] = journeys_enriched['duration'].astype(int)\n",
    "        journeys_enriched['journey_id'] = journeys_enriched.index\n",
    "        #journeys_enriched = journeys_enriched[['journey_id', 'started_at', 'ended_at', 'duration', 'start_neighbourhood', 'end_neighbourhood', 'day_of_week', 'hour_of_day', 'month_of_year']]\n",
    "        self.df_to_db('journeys_enriched', journeys_enriched)\n",
    "\n",
    "class BlueBikesVisual:\n",
    "\n",
    "    def _init__(self):\n",
    "        pass\n",
    "\n",
    "    def create_dash_application(self, response_variable='journeys', grouping_variable='start_neighbourhood'):\n",
    "        self.engine = create_engine('postgresql://postgres:postgres@localhost:5432/desmondmolloy')\n",
    "        # Create a connection to the engine called `conn`\n",
    "        self.conn = self.engine.connect()\n",
    "        # Create the Dash app\n",
    "        app = dash.Dash(__name__)\n",
    "        # Create a DataFrame from the Postgres table\n",
    "        assert response_variable in ['journeys', 'duration']\n",
    "        if response_variable == 'journeys':\n",
    "            df = pd.read_sql('SELECT {}, COUNT(*) as journeys FROM journeys_enriched group by 1'.format(grouping_variable), self.conn)\n",
    "        else:\n",
    "            df = pd.read_sql('SELECT {}, AVG(duration) as duration FROM journeys_enriched group by 1'.format(grouping_variable), self.conn)\n",
    "        # Create a bar chart of the number of trips by neighbourhood\n",
    "        fig = px.bar(df, x=grouping_variable, y=response_variable)\n",
    "        # Create the Dash app layout\n",
    "        app.layout = html.Div(children=[\n",
    "            html.H1(children='Hello Dash'),\n",
    "            dcc.Graph(\n",
    "                id='example-graph',\n",
    "                figure=fig\n",
    "            )\n",
    "        ])\n",
    "        # Return the app\n",
    "        return app\n",
    "    \n",
    "    def run(self):\n",
    "        app = self.create_dash_application()\n",
    "        app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/7wfwn_sn3cv5zpj5dk7k9r3r0000gn/T/ipykernel_2328/3071158152.py:62: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('(', '')\n",
      "/var/folders/63/7wfwn_sn3cv5zpj5dk7k9r3r0000gn/T/ipykernel_2328/3071158152.py:63: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace(')', '')\n",
      "/var/folders/63/7wfwn_sn3cv5zpj5dk7k9r3r0000gn/T/ipykernel_2328/3071158152.py:64: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  poly_df['neighbourhood'] = poly_df['neighbourhood'].str.replace('.', '')\n",
      "/var/folders/63/7wfwn_sn3cv5zpj5dk7k9r3r0000gn/T/ipykernel_2328/3071158152.py:75: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  stations['station_name'] = stations['station_name'].str.replace('(', '')\n",
      "/var/folders/63/7wfwn_sn3cv5zpj5dk7k9r3r0000gn/T/ipykernel_2328/3071158152.py:76: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  stations['station_name'] = stations['station_name'].str.replace(')', '')\n",
      "/var/folders/63/7wfwn_sn3cv5zpj5dk7k9r3r0000gn/T/ipykernel_2328/3071158152.py:77: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  stations['station_name'] = stations['station_name'].str.replace('.', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m * Tip: There are .env or .flaskenv files present. Do \"pip install python-dotenv\" to use them.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "/Users/desmondmolloy/Documents/GitHub/BikeShareAI/.conda/lib/python3.11/site-packages/traitlets/traitlets.py:2548: FutureWarning: Supporting extra quotes around strings is deprecated in traitlets 5.0. You can use 'hmac-sha256' instead of '\"hmac-sha256\"' if you require traitlets >=5.\n",
      "  warn(\n",
      "/Users/desmondmolloy/Documents/GitHub/BikeShareAI/.conda/lib/python3.11/site-packages/traitlets/traitlets.py:2499: FutureWarning: Supporting extra quotes around Bytes is deprecated in traitlets 5.0. Use '1d7f7f9f-b34d-4749-8217-e83e2030ca47' instead of 'b\"1d7f7f9f-b34d-4749-8217-e83e2030ca47\"'.\n",
      "  warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/desmondmolloy/Documents/GitHub/BikeShareAI/.conda/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/desmondmolloy/Documents/GitHub/BikeShareAI/.conda/lib/python3.11/site-packages/traitlets/config/application.py\", line 1042, in launch_instance\n",
      "    app.initialize(argv)\n",
      "  File \"/Users/desmondmolloy/Documents/GitHub/BikeShareAI/.conda/lib/python3.11/site-packages/traitlets/config/application.py\", line 113, in inner\n",
      "    return method(app, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/desmondmolloy/Documents/GitHub/BikeShareAI/.conda/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 678, in initialize\n",
      "    self.init_sockets()\n",
      "  File \"/Users/desmondmolloy/Documents/GitHub/BikeShareAI/.conda/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 317, in init_sockets\n",
      "    self.shell_port = self._bind_socket(self.shell_socket, self.shell_port)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/desmondmolloy/Documents/GitHub/BikeShareAI/.conda/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 252, in _bind_socket\n",
      "    return self._try_bind_socket(s, port)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/desmondmolloy/Documents/GitHub/BikeShareAI/.conda/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 228, in _try_bind_socket\n",
      "    s.bind(\"tcp://%s:%i\" % (self.ip, port))\n",
      "  File \"/Users/desmondmolloy/Documents/GitHub/BikeShareAI/.conda/lib/python3.11/site-packages/zmq/sugar/socket.py\", line 301, in bind\n",
      "    super().bind(addr)\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 564, in zmq.backend.cython.socket.Socket.bind\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 28, in zmq.backend.cython.checkrc._check_rc\n",
      "zmq.error.ZMQError: Address already in use\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/desmondmolloy/Documents/GitHub/BikeShareAI/.conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3516: UserWarning:\n",
      "\n",
      "To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the pipeline\n",
    "pipeline = BlueBikesDataPipeline('https://s3.amazonaws.com/hubway-data/202304-bluebikes-tripdata.zip')\n",
    "pipeline.unzip_file_to_local_csv()\n",
    "pipeline.main_join('data/Boston_Neighborhoods.geojson', 'data/current_bluebikes_stations.csv', 'data/202304-bluebikes-tripdata.csv')\n",
    "visual = BlueBikesVisual()\n",
    "visual.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28.05.2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When work was picked back up on 28-05-2023, a different approach to writing design specifications was adapted. To keep from being hemmed in or limited by the visible, existing code, a separate textfile was used to write a specification for the dashboard script, at dashboard_20230528.txt. This was then copied into the notebook, and the code was written to match the specification. Please note that Copilot wrote the last sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a class called DashboardBikes with the following methods:\n",
    "\n",
    "# create_dash_application: Creates a Dash application with one bar graph, and two instances of dcc.Dropdown. \n",
    "# The first instance of dcc.Dropdown should be called response_variable\n",
    "# The second instance should be called grouping_variable\n",
    "\n",
    "# If response_variable = journeys, then pass a SQL statement that counts all records by grouping_variable\n",
    "# If response_variable = duration, then pass a SQL statement that selects the mean duration group by grouping variable\n",
    "\n",
    "# Grouping variable should be in the following:\n",
    "# start_neighbourhood\n",
    "# end_neighbourhood\n",
    "# day_of_week\n",
    "# hour_of_day\n",
    "# month_of_year\n",
    "# The application should have a bar graph showing the response variable by grouping variable\n",
    "\n",
    "class DashboardBikes:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def create_dash_application(self, response_variable='journeys', grouping_variable='start_neighbourhood'):\n",
    "        self.engine = create_engine('postgresql://postgres:postgres@localhost:5432/desmondmolloy')\n",
    "        # Create a connection to the engine called `conn`\n",
    "        self.conn = self.engine.connect()\n",
    "        # Create the Dash app\n",
    "        app = dash.Dash(__name__)\n",
    "        # Create a DataFrame from the Postgres table\n",
    "        assert response_variable in ['journeys', 'duration']\n",
    "        if response_variable == 'journeys':\n",
    "            df = pd.read_sql('SELECT {}, COUNT(*) as journeys FROM journeys_enriched group by 1'.format(grouping_variable), self.conn)\n",
    "        else:\n",
    "            df = pd.read_sql('SELECT {}, AVG(duration) as duration FROM journeys_enriched group by 1'.format(grouping_variable), self.conn)\n",
    "        # Create a bar chart of the number of trips by neighbourhood\n",
    "        fig = px.bar(df, x=grouping_variable, y=response_variable)\n",
    "        # Create the Dash app layout\n",
    "        app.layout = html.Div(children=[\n",
    "            html.H1(children='Hello Dash'),\n",
    "            dcc.Graph(\n",
    "                id='example-graph',\n",
    "                figure=fig\n",
    "            )\n",
    "        ])\n",
    "        # Return the app\n",
    "        return app"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, Copilot simply copied the earlier code, including its flaws (such as a lack of dropdown). We therefore had to paste the prompt into a different notebook, to escape the local bias.\n",
    "\n",
    "This approach was successful, returning the following code:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Write a class called DashboardBikes with the following methods:\n",
    "\n",
    "# create_dash_application: Creates a Dash application with one bar graph, and two instances of dcc.Dropdown. \n",
    "# The first instance of dcc.Dropdown should be called response_variable\n",
    "# The second instance should be called grouping_variable\n",
    "\n",
    "# If response_variable = journeys, then pass a SQL statement that counts all records by grouping_variable\n",
    "# If response_variable = duration, then pass a SQL statement that selects the mean duration group by grouping variable\n",
    "\n",
    "# Grouping variable should be in the following:\n",
    "# start_neighbourhood\n",
    "# end_neighbourhood\n",
    "# day_of_week\n",
    "# hour_of_day\n",
    "# month_of_year\n",
    "# The application should have a bar graph showing the response variable by grouping variable\n",
    "\n",
    "# The application should have a title called \"Bike Dashboard\"\n",
    "\n",
    "# The application should have a subtitle called \"Created by Desmond Molloy\"\n",
    "\n",
    "class DashboardBikes:\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "    def create_dash_application(self):\n",
    "        \n",
    "        app = dash.Dash(__name__)\n",
    "        \n",
    "        app.layout = html.Div([\n",
    "            html.H1('Bike Dashboard'),\n",
    "            html.H2('Created by Desmond Molloy'),\n",
    "            dcc.Dropdown(\n",
    "                id='response_variable',\n",
    "                options=[\n",
    "                    {'label': 'Journeys', 'value': 'journeys'},\n",
    "                    {'label': 'Duration', 'value': 'duration'}\n",
    "                ],\n",
    "                value='journeys'\n",
    "            ),\n",
    "            dcc.Dropdown(\n",
    "                id='grouping_variable',\n",
    "                options=[\n",
    "                    {'label': 'Start Neighbourhood', 'value': 'start_neighbourhood'},\n",
    "                    {'label': 'End Neighbourhood', 'value': 'end_neighbourhood'},\n",
    "                    {'label': 'Day of Week', 'value': 'day_of_week'},\n",
    "                    {'label': 'Hour of Day', 'value': 'hour_of_day'},\n",
    "                    {'label': 'Month of Year', 'value': 'month_of_year'}\n",
    "                ],\n",
    "                value='start_neighbourhood'\n",
    "            ),\n",
    "            dcc.Graph(id='bike_graph')\n",
    "        ])\n",
    "        \n",
    "        @app.callback(\n",
    "            Output('bike_graph', 'figure'),\n",
    "            Input('response_variable', 'value'),\n",
    "            Input('grouping_variable', 'value'))\n",
    "        def update_graph(response_variable, grouping_variable):\n",
    "            if response_variable == 'journeys':\n",
    "                df = self.df.groupby(grouping_variable).count().reset_index()\n",
    "                fig = px.bar(df, x=grouping_variable, y='journey_id')\n",
    "            elif response_variable == 'duration':\n",
    "                df = self.df.groupby(grouping_variable).mean().reset_index()\n",
    "                fig = px.bar(df, x=grouping_variable, y='duration')\n",
    "            return fig\n",
    "        \n",
    "        return app\n",
    "    \n",
    "    def run_dash_application(self, app):\n",
    "        app.run_server(debug=True, use_reloader=False)\n",
    "    \n",
    "# Path: sub_benches/outbench_20230528.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DashboardBikes.__init__() missing 1 required positional argument: 'df'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbikeshareai\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbluebikes\u001b[39;00m \u001b[39mimport\u001b[39;00m BlueBikesDataPipeline, DashboardBikes\n\u001b[1;32m      2\u001b[0m \u001b[39m#Launch the dashboard\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m dashboard \u001b[39m=\u001b[39m DashboardBikes()\n\u001b[1;32m      4\u001b[0m dashboard\u001b[39m.\u001b[39mcreate_dash_application()\n\u001b[1;32m      5\u001b[0m dashboard\u001b[39m.\u001b[39mrun()\n",
      "\u001b[0;31mTypeError\u001b[0m: DashboardBikes.__init__() missing 1 required positional argument: 'df'"
     ]
    }
   ],
   "source": [
    "from bikeshareai.bluebikes import BlueBikesDataPipeline, DashboardBikes\n",
    "#Launch the dashboard\n",
    "dashboard = DashboardBikes()\n",
    "dashboard.create_dash_application()\n",
    "dashboard.run()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was necessary to specify that there should be no df argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a class called DashboardBike with the following methods:\n",
    "\n",
    "# - __init__: Takes no arguments. Creates a Dash application with one bar graph, and two instances of dcc.Dropdown.\n",
    "\n",
    "# The first instance of dcc.Dropdown should be called response_variable\n",
    "# The second instance should be called grouping_variable\n",
    "\n",
    "# If response_variable = journeys, then pass a SQL statement that counts all records by grouping_variable\n",
    "# If response_variable = duration, then pass a SQL statement that selects the mean duration group by grouping variable\n",
    "\n",
    "# Grouping variable should be in the following:\n",
    "# start_neighbourhood\n",
    "# end_neighbourhood\n",
    "# day_of_week\n",
    "# hour_of_day\n",
    "# month_of_year\n",
    "# The application should have a bar graph showing the response variable by grouping variable\n",
    "\n",
    "# - run: Calls create_dash_application and runs the Dash application on the local server\n",
    "\n",
    "# - get_sql: Takes no arguments. Returns a SQL statement that counts all records by grouping_variable\n",
    "\n",
    "# - get_sql2: Takes no arguments. Returns a SQL statement that selects the mean duration group by grouping variable\n",
    "\n",
    "class DashboardBike:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.app = dash.Dash(__name__)\n",
    "        self.app.layout = html.Div([\n",
    "            html.H1('Bike Dashboard'),\n",
    "            html.H2('Created by Desmond Molloy'),\n",
    "            dcc.Dropdown(\n",
    "                id='response_variable',\n",
    "                options=[\n",
    "                    {'label': 'Journeys', 'value': 'journeys'},\n",
    "                    {'label': 'Duration', 'value': 'duration'}\n",
    "                ],\n",
    "                value='journeys'\n",
    "            ),\n",
    "            dcc.Dropdown(\n",
    "                id='grouping_variable',\n",
    "                options=[\n",
    "                    {'label': 'Start Neighbourhood', 'value': 'start_neighbourhood'},\n",
    "                    {'label': 'End Neighbourhood', 'value': 'end_neighbourhood'},\n",
    "                    {'label': 'Day of Week', 'value': 'day_of_week'},\n",
    "                    {'label': 'Hour of Day', 'value': 'hour_of_day'},\n",
    "                    {'label': 'Month of Year', 'value': 'month_of_year'}\n",
    "                ],\n",
    "                value='start_neighbourhood'\n",
    "            ),\n",
    "            dcc.Graph(id='bike_graph')\n",
    "        ])\n",
    "            \n",
    "        @self.app.callback(\n",
    "            Output('bike_graph', 'figure'),\n",
    "            Input('response_variable', 'value'),\n",
    "            Input('grouping_variable', 'value'))\n",
    "        def update_graph(response_variable, grouping_variable):\n",
    "            if response_variable == 'journeys':\n",
    "                sql = self.get_sql(grouping_variable)\n",
    "            else:\n",
    "                sql = self.get_sql2(grouping_variable)\n",
    "            df = pd.read_sql(sql, con=engine)\n",
    "            fig = px.bar(df, x=grouping_variable, y='count')\n",
    "            return fig\n",
    "\n",
    "    def run(self):\n",
    "        self.app.run_server(debug=True, use_reloader=False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dash' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#Create an instance of the DashboardBike class and run the run method\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dashboard \u001b[39m=\u001b[39m DashboardBike()\n\u001b[1;32m      3\u001b[0m dashboard\u001b[39m.\u001b[39mrun()\n",
      "Cell \u001b[0;32mIn[5], line 28\u001b[0m, in \u001b[0;36mDashboardBike.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 28\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapp \u001b[39m=\u001b[39m dash\u001b[39m.\u001b[39mDash(\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     29\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapp\u001b[39m.\u001b[39mlayout \u001b[39m=\u001b[39m html\u001b[39m.\u001b[39mDiv([\n\u001b[1;32m     30\u001b[0m         html\u001b[39m.\u001b[39mH1(\u001b[39m'\u001b[39m\u001b[39mBike Dashboard\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     31\u001b[0m         html\u001b[39m.\u001b[39mH2(\u001b[39m'\u001b[39m\u001b[39mCreated by Desmond Molloy\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m         dcc\u001b[39m.\u001b[39mGraph(\u001b[39mid\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbike_graph\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     52\u001b[0m     ])\n\u001b[1;32m     54\u001b[0m     \u001b[39m@self\u001b[39m\u001b[39m.\u001b[39mapp\u001b[39m.\u001b[39mcallback(\n\u001b[1;32m     55\u001b[0m         Output(\u001b[39m'\u001b[39m\u001b[39mbike_graph\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfigure\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     56\u001b[0m         Input(\u001b[39m'\u001b[39m\u001b[39mresponse_variable\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     57\u001b[0m         Input(\u001b[39m'\u001b[39m\u001b[39mgrouping_variable\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     58\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mupdate_graph\u001b[39m(response_variable, grouping_variable):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dash' is not defined"
     ]
    }
   ],
   "source": [
    "#Create an instance of the DashboardBike class and run the run method\n",
    "dashboard = DashboardBike()\n",
    "dashboard.run()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needed imports had to be added to the design specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/7wfwn_sn3cv5zpj5dk7k9r3r0000gn/T/ipykernel_2744/2044842085.py:30: UserWarning: \n",
      "The dash_core_components package is deprecated. Please replace\n",
      "`import dash_core_components as dcc` with `from dash import dcc`\n",
      "  import dash_core_components as dcc\n",
      "/var/folders/63/7wfwn_sn3cv5zpj5dk7k9r3r0000gn/T/ipykernel_2744/2044842085.py:31: UserWarning: \n",
      "The dash_html_components package is deprecated. Please replace\n",
      "`import dash_html_components as html` with `from dash import html`\n",
      "  import dash_html_components as html\n"
     ]
    }
   ],
   "source": [
    "# Write a class called DashboardBike with the following methods:\n",
    "\n",
    "# - __init__: Takes no arguments. Creates a Dash application with one bar graph, and two instances of dcc.Dropdown.\n",
    "\n",
    "# The first instance of dcc.Dropdown should be called response_variable\n",
    "# The second instance should be called grouping_variable\n",
    "\n",
    "# If response_variable = journeys, then pass a SQL statement that counts all records by grouping_variable\n",
    "# If response_variable = duration, then pass a SQL statement that selects the mean duration group by grouping variable\n",
    "\n",
    "# Grouping variable should be in the following:\n",
    "# start_neighbourhood\n",
    "# end_neighbourhood\n",
    "# day_of_week\n",
    "# hour_of_day\n",
    "# month_of_year\n",
    "# The application should have a bar graph showing the response variable by grouping variable\n",
    "\n",
    "# - run: Calls create_dash_application and runs the Dash application on the local server\n",
    "\n",
    "# - get_sql: Takes no arguments. Returns a SQL statement that counts all records by grouping_variable\n",
    "\n",
    "# - get_sql2: Takes no arguments. Returns a SQL statement that selects the mean duration group by grouping variable\n",
    "\n",
    "#First import all needed libraries\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "\n",
    "#Then create the class\n",
    "\n",
    "class DashboardBike:\n",
    "    \n",
    "        def __init__(self):\n",
    "            self.app = dash.Dash(__name__)\n",
    "            self.app.layout = html.Div([\n",
    "                html.H1('Bike Dashboard'),\n",
    "                html.H2('Created by Desmond Molloy'),\n",
    "                dcc.Dropdown(\n",
    "                    id='response_variable',\n",
    "                    options=[\n",
    "                        {'label': 'Journeys', 'value': 'journeys'},\n",
    "                        {'label': 'Duration', 'value': 'duration'}\n",
    "                    ],\n",
    "                    value='journeys'\n",
    "                ),\n",
    "                dcc.Dropdown(\n",
    "                    id='grouping_variable',\n",
    "                    options=[\n",
    "                        {'label': 'Start Neighbourhood', 'value': 'start_neighbourhood'},\n",
    "                        {'label': 'End Neighbourhood', 'value': 'end_neighbourhood'},\n",
    "                        {'label': 'Day of Week', 'value': 'day_of_week'},\n",
    "                        {'label': 'Hour of Day', 'value': 'hour_of_day'},\n",
    "                        {'label': 'Month of Year', 'value': 'month_of_year'}\n",
    "                    ],\n",
    "                    value='start_neighbourhood'\n",
    "                ),\n",
    "                dcc.Graph(id='bike_graph')\n",
    "            ])\n",
    "                \n",
    "            @self.app.callback(\n",
    "                Output('bike_graph', 'figure'),\n",
    "                Input('response_variable', 'value'),\n",
    "                Input('grouping_variable', 'value'))\n",
    "            def update_graph(response_variable, grouping_variable):\n",
    "                if response_variable == 'journeys':\n",
    "                    sql = self.get_sql(grouping_variable)\n",
    "                else:\n",
    "                    sql = self.get_sql2(grouping_variable)\n",
    "                df = pd.read_sql(sql, con=engine)\n",
    "                fig = px.bar(df, x=grouping_variable, y='count')\n",
    "                return fig\n",
    "    \n",
    "        def run(self):\n",
    "            self.app.run_server(debug=True, use_reloader=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m * Tip: There are .env or .flaskenv files present. Do \"pip install python-dotenv\" to use them.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    }
   ],
   "source": [
    "#Call the class and run the run method\n",
    "\n",
    "dashboard = DashboardBike()\n",
    "dashboard.run()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The design had to be rewritten due to the absence of the get_sql and get_sql2 methods. The following code was used instead:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/7wfwn_sn3cv5zpj5dk7k9r3r0000gn/T/ipykernel_3520/3994583546.py:4: UserWarning: \n",
      "The dash_core_components package is deprecated. Please replace\n",
      "`import dash_core_components as dcc` with `from dash import dcc`\n",
      "  import dash_core_components as dcc\n",
      "/var/folders/63/7wfwn_sn3cv5zpj5dk7k9r3r0000gn/T/ipykernel_3520/3994583546.py:5: UserWarning: \n",
      "The dash_html_components package is deprecated. Please replace\n",
      "`import dash_html_components as html` with `from dash import html`\n",
      "  import dash_html_components as html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "class DashboardBike:\n",
    "\n",
    "    def __init__(self):\n",
    "        # The line below was added manually\n",
    "        self.engine = create_engine('postgresql://postgres:postgres@localhost:5432/desmondmolloy')\n",
    "        self.app = dash.Dash(__name__)\n",
    "        self.app.layout = html.Div([\n",
    "            html.H1('Bike Dashboard'),\n",
    "            html.H2('Created by Desmond Molloy'),\n",
    "            dcc.Dropdown(\n",
    "                id='response_variable',\n",
    "                options=[\n",
    "                    {'label': 'Journeys', 'value': 'journeys'},\n",
    "                    {'label': 'Duration', 'value': 'duration'}\n",
    "                ],\n",
    "                value='journeys'\n",
    "            ),\n",
    "            dcc.Dropdown(\n",
    "                id='grouping_variable',\n",
    "                options=[\n",
    "                    {'label': 'Start Neighbourhood', 'value': 'start_neighbourhood'},\n",
    "                    {'label': 'End Neighbourhood', 'value': 'end_neighbourhood'},\n",
    "                    {'label': 'Day of Week', 'value': 'day_of_week'},\n",
    "                    {'label': 'Hour of Day', 'value': 'hour_of_day'},\n",
    "                    {'label': 'Month of Year', 'value': 'month_of_year'}\n",
    "                ],\n",
    "                value='start_neighbourhood'\n",
    "            ),\n",
    "            dcc.Graph(id='bike_graph')\n",
    "        ])\n",
    "\n",
    "        @self.app.callback(\n",
    "            Output('bike_graph', 'figure'),\n",
    "            Input('response_variable', 'value'),\n",
    "            Input('grouping_variable', 'value'))\n",
    "        def update_graph(response_variable, grouping_variable):\n",
    "            if response_variable == 'journeys':\n",
    "                sql = 'SELECT {}, COUNT(*) as count FROM journeys_enriched group by 1'.format(grouping_variable)\n",
    "            else:\n",
    "                sql = 'SELECT {}, AVG(duration) as count FROM journeys_enriched group by 1'.format(grouping_variable)\n",
    "            df = pd.read_sql(sql, con=self.engine)\n",
    "            fig = px.bar(df, x=grouping_variable, y='count')\n",
    "            return fig\n",
    "        \n",
    "    def run(self):\n",
    "        self.app.run_server(debug=True, use_reloader=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m * Tip: There are .env or .flaskenv files present. Do \"pip install python-dotenv\" to use them.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    }
   ],
   "source": [
    "#Call the class and run the run method\n",
    "\n",
    "dashboard = DashboardBike()\n",
    "dashboard.run()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this occasion, the dashboard ran exactly as desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BlueBikesDataPipeline' object has no attribute 'main_join'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m pipeline \u001b[39m=\u001b[39m BlueBikesDataPipeline(\u001b[39m'\u001b[39m\u001b[39mhttps://s3.amazonaws.com/hubway-data/202304-bluebikes-tripdata.zip\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m pipeline\u001b[39m.\u001b[39munzip_file_to_local_csv()\n\u001b[0;32m----> 5\u001b[0m pipeline\u001b[39m.\u001b[39;49mmain_join(\u001b[39m'\u001b[39m\u001b[39mdata/Boston_Neighborhoods.geojson\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdata/current_bluebikes_stations.csv\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdata/202304-bluebikes-tripdata.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m pipeline\u001b[39m.\u001b[39menrich_journeys()\n\u001b[1;32m      7\u001b[0m \u001b[39m# Create an instance of DashboardBike and run the run method\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BlueBikesDataPipeline' object has no attribute 'main_join'"
     ]
    }
   ],
   "source": [
    "from bikeshareai.bluebikes import BlueBikesDataPipeline, DashboardBike\n",
    "# Create an instance of BlueBikesDataPipeline and run the methods\n",
    "pipeline = BlueBikesDataPipeline('https://s3.amazonaws.com/hubway-data/202304-bluebikes-tripdata.zip')\n",
    "pipeline.unzip_file_to_local_csv()\n",
    "pipeline.main_join('data/Boston_Neighborhoods.geojson', 'data/current_bluebikes_stations.csv', 'data/202304-bluebikes-tripdata.csv')\n",
    "pipeline.enrich_journeys()\n",
    "# Create an instance of DashboardBike and run the run method\n",
    "dashboard = DashboardBike()\n",
    "dashboard.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a README text string for this repository\n",
    "readme_text = \"\"\"\n",
    "# Blue Bikes Data Pipeline\n",
    "\n",
    "This repository contains a data pipeline for the Blue Bikes bike sharing system in Boston. The pipeline downloads the data from the Blue Bikes website, joins it to a shapefile of Boston neighbourhoods, and then loads it into a PostgreSQL database. It also contains a dashboard that allows the user to visualise the data.\n",
    "\n",
    "## Installation\n",
    "\n",
    "To install the package, run the following command in the terminal:\n",
    "\n",
    "```\n",
    "pip install git+\n",
    "```\n",
    "\n",
    "## Usage\n",
    "\n",
    "To use the package, run the following commands in the terminal:\n",
    "\n",
    "```\n",
    "from bikeshareai.bluebikes import BlueBikesDataPipeline, DashboardBike\n",
    "# Create an instance of BlueBikesDataPipeline and run the methods\n",
    "pipeline = BlueBikesDataPipeline('https://s3.amazonaws.com/hubway-data/202304-bluebikes-tripdata.zip')\n",
    "pipeline.unzip_file_to_local_csv()\n",
    "pipeline.main_join('data/Boston_Neighborhoods.geojson', 'data/current_bluebikes_stations.csv', 'data/202304-bluebikes-tripdata.csv')\n",
    "pipeline.enrich_journeys()\n",
    "# Create an instance of DashboardBike and run the run method\n",
    "dashboard = DashboardBike()\n",
    "dashboard.run()\n",
    "```\n",
    "\n",
    "## License\n",
    "\n",
    "This package is licensed under the MIT License. See the LICENSE file for details.\n",
    "\n",
    "## Credits\n",
    "\n",
    "This package was created by Desmond Molloy.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Write the README string to a file\n",
    "with open('README.md', 'w') as f:\n",
    "    f.write(readme_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Import all functions from bikeshareai.bluebikes module, call the pipeline and create the Dash app\n",
    "from bikeshareai.bluebikes import BlueBikesDataPipeline, DashboardBike\n",
    "\n",
    "# Create an instance of the BlueBikesDataPipeline class\n",
    "pipeline = BlueBikesDataPipeline('https://s3.amazonaws.com/hubway-data/201705-bluebikes-tripdata.zip')\n",
    "# Connect to the database\n",
    "pipeline.connect_to_db()\n",
    "# Unzip the file and save the CSV locally\n",
    "pipeline.unzip_file_to_local_csv()\n",
    "# Create the `stations` table in the database\n",
    "pipeline.csv_to_db('stations', 'data/current_bluebikes_stations.csv')\n",
    "pipeline.enrich_journeys('data/Boston_Neighborhoods.geojson','data/202304-bluebikes-tripdata.csv')\n",
    "\n",
    "# Create an instance of the DashboardBike class\n",
    "app = DashboardBike()\n",
    "# Run the Dash app\n",
    "app.run_server(debug=True)\n",
    "\n",
    "# Path: outbench_20230528.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
